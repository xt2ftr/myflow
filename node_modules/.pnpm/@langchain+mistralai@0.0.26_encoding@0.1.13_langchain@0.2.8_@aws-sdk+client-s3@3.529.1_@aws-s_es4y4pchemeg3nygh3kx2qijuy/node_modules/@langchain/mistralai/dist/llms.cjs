"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.MistralAI = void 0;
const llms_1 = require("@langchain/core/language_models/llms");
const outputs_1 = require("@langchain/core/outputs");
const env_1 = require("@langchain/core/utils/env");
const chunk_array_1 = require("@langchain/core/utils/chunk_array");
const async_caller_1 = require("@langchain/core/utils/async_caller");
/**
 * MistralAI completions LLM.
 */
class MistralAI extends llms_1.LLM {
    static lc_name() {
        return "MistralAI";
    }
    constructor(fields) {
        super(fields ?? {});
        Object.defineProperty(this, "model", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "codestral-latest"
        });
        Object.defineProperty(this, "temperature", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0
        });
        Object.defineProperty(this, "topP", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "maxTokens", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "randomSeed", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "streaming", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "batchSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 20
        });
        Object.defineProperty(this, "apiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "endpoint", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "maxRetries", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "maxConcurrency", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        this.model = fields?.model ?? this.model;
        this.temperature = fields?.temperature ?? this.temperature;
        this.topP = fields?.topP ?? this.topP;
        this.maxTokens = fields?.maxTokens ?? this.maxTokens;
        this.randomSeed = fields?.randomSeed ?? this.randomSeed;
        this.batchSize = fields?.batchSize ?? this.batchSize;
        this.streaming = fields?.streaming ?? this.streaming;
        this.endpoint = fields?.endpoint;
        this.maxRetries = fields?.maxRetries;
        this.maxConcurrency = fields?.maxConcurrency;
        const apiKey = fields?.apiKey ?? (0, env_1.getEnvironmentVariable)("MISTRAL_API_KEY");
        if (!apiKey) {
            throw new Error(`MistralAI requires an API key to be set.
Either provide one via the "apiKey" field in the constructor, or set the "MISTRAL_API_KEY" environment variable.`);
        }
        this.apiKey = apiKey;
    }
    get lc_secrets() {
        return {
            apiKey: "MISTRAL_API_KEY",
        };
    }
    _llmType() {
        return "mistralai";
    }
    invocationParams(options) {
        return {
            model: this.model,
            suffix: options.suffix,
            temperature: this.temperature,
            maxTokens: this.maxTokens,
            topP: this.topP,
            randomSeed: this.randomSeed,
            stop: options.stop,
        };
    }
    /**
     * For some given input string and options, return a string output.
     *
     * Despite the fact that `invoke` is overridden below, we still need this
     * in order to handle public APi calls to `generate()`.
     */
    async _call(prompt, options) {
        const params = {
            ...this.invocationParams(options),
            prompt,
        };
        const result = await this.completionWithRetry(params, options, false);
        return result.choices[0].message.content ?? "";
    }
    async _generate(prompts, options, runManager) {
        const subPrompts = (0, chunk_array_1.chunkArray)(prompts, this.batchSize);
        const choices = [];
        const params = this.invocationParams(options);
        for (let i = 0; i < subPrompts.length; i += 1) {
            const data = await (async () => {
                if (this.streaming) {
                    const responseData = [];
                    for (let x = 0; x < subPrompts[i].length; x += 1) {
                        const choices = [];
                        let response;
                        const stream = await this.completionWithRetry({
                            ...params,
                            prompt: subPrompts[i][x],
                        }, options, true);
                        for await (const message of stream) {
                            // on the first message set the response properties
                            if (!response) {
                                response = {
                                    id: message.id,
                                    object: "chat.completion",
                                    created: message.created,
                                    model: message.model,
                                };
                            }
                            // on all messages, update choice
                            for (const part of message.choices) {
                                if (!choices[part.index]) {
                                    choices[part.index] = {
                                        index: part.index,
                                        message: {
                                            role: part.delta.role ?? "assistant",
                                            content: part.delta.content ?? "",
                                            tool_calls: null,
                                        },
                                        finish_reason: part.finish_reason,
                                    };
                                }
                                else {
                                    const choice = choices[part.index];
                                    choice.message.content += part.delta.content ?? "";
                                    choice.finish_reason = part.finish_reason;
                                }
                                void runManager?.handleLLMNewToken(part.delta.content ?? "", {
                                    prompt: part.index,
                                    completion: part.index,
                                });
                            }
                        }
                        if (options.signal?.aborted) {
                            throw new Error("AbortError");
                        }
                        responseData.push({
                            ...response,
                            choices,
                        });
                    }
                    return responseData;
                }
                else {
                    const responseData = [];
                    for (let x = 0; x < subPrompts[i].length; x += 1) {
                        const res = await this.completionWithRetry({
                            ...params,
                            prompt: subPrompts[i][x],
                        }, options, false);
                        responseData.push(res);
                    }
                    return responseData;
                }
            })();
            choices.push(...data.map((d) => d.choices));
        }
        const generations = choices.map((promptChoices) => promptChoices.map((choice) => ({
            text: choice.message.content ?? "",
            generationInfo: {
                finishReason: choice.finish_reason,
            },
        })));
        return {
            generations,
        };
    }
    async completionWithRetry(request, options, stream) {
        const { MistralClient } = await this.imports();
        const caller = new async_caller_1.AsyncCaller({
            maxConcurrency: options.maxConcurrency || this.maxConcurrency,
            maxRetries: this.maxRetries,
        });
        const client = new MistralClient(this.apiKey, this.endpoint, this.maxRetries, options.timeout);
        return caller.callWithOptions({
            signal: options.signal,
        }, async () => {
            if (stream) {
                return client.completionStream(request);
            }
            else {
                return client.completion(request);
            }
        });
    }
    async *_streamResponseChunks(prompt, options, runManager) {
        const params = {
            ...this.invocationParams(options),
            prompt,
        };
        const stream = await this.completionWithRetry(params, options, true);
        for await (const data of stream) {
            const choice = data?.choices[0];
            if (!choice) {
                continue;
            }
            const chunk = new outputs_1.GenerationChunk({
                text: choice.delta.content ?? "",
                generationInfo: {
                    finishReason: choice.finish_reason,
                    tokenUsage: data.usage,
                },
            });
            yield chunk;
            // eslint-disable-next-line no-void
            void runManager?.handleLLMNewToken(chunk.text ?? "");
        }
        if (options.signal?.aborted) {
            throw new Error("AbortError");
        }
    }
    /** @ignore */
    async imports() {
        const { default: MistralClient } = await import("@mistralai/mistralai");
        return { MistralClient };
    }
}
exports.MistralAI = MistralAI;
