import { ChatCompletionResponse, Function as MistralAIFunction, ChatCompletionResponseChunk, ChatRequest, Tool as MistralAITool } from "@mistralai/mistralai";
import { type BaseMessage, AIMessageChunk } from "@langchain/core/messages";
import type { BaseLanguageModelInput, BaseLanguageModelCallOptions, StructuredOutputMethodParams, StructuredOutputMethodOptions } from "@langchain/core/language_models/base";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { type BaseChatModelParams, BaseChatModel, LangSmithParams } from "@langchain/core/language_models/chat_models";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { StructuredToolInterface } from "@langchain/core/tools";
import { z } from "zod";
import { Runnable, RunnableToolLike } from "@langchain/core/runnables";
export type MistralAIToolChoice = "auto" | "any" | "none";
type MistralAIToolInput = {
    type: string;
    function: MistralAIFunction;
};
interface MistralAICallOptions extends Omit<BaseLanguageModelCallOptions, "stop"> {
    response_format?: {
        type: "text" | "json_object";
    };
    tools: StructuredToolInterface[] | MistralAIToolInput[] | MistralAITool[];
    tool_choice?: MistralAIToolChoice;
    /**
     * Whether or not to include token usage in the stream.
     * @default {true}
     */
    streamUsage?: boolean;
}
export interface ChatMistralAICallOptions extends MistralAICallOptions {
}
/**
 * Input to chat model class.
 */
export interface ChatMistralAIInput extends BaseChatModelParams, Pick<ChatMistralAICallOptions, "streamUsage"> {
    /**
     * The API key to use.
     * @default {process.env.MISTRAL_API_KEY}
     */
    apiKey?: string;
    /**
     * The name of the model to use.
     * Alias for `model`
     * @default {"mistral-small-latest"}
     */
    modelName?: string;
    /**
     * The name of the model to use.
     * @default {"mistral-small-latest"}
     */
    model?: string;
    /**
     * Override the default endpoint.
     */
    endpoint?: string;
    /**
     * What sampling temperature to use, between 0.0 and 2.0.
     * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     * @default {0.7}
     */
    temperature?: number;
    /**
     * Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass.
     * So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     * Should be between 0 and 1.
     * @default {1}
     */
    topP?: number;
    /**
     * The maximum number of tokens to generate in the completion.
     * The token count of your prompt plus max_tokens cannot exceed the model's context length.
     */
    maxTokens?: number;
    /**
     * Whether or not to stream the response.
     * @default {false}
     */
    streaming?: boolean;
    /**
     * Whether to inject a safety prompt before all conversations.
     * @default {false}
     * @deprecated use safePrompt instead
     */
    safeMode?: boolean;
    /**
     * Whether to inject a safety prompt before all conversations.
     * @default {false}
     */
    safePrompt?: boolean;
    /**
     * The seed to use for random sampling. If set, different calls will generate deterministic results.
     * Alias for `seed`
     */
    randomSeed?: number;
    /**
     * The seed to use for random sampling. If set, different calls will generate deterministic results.
     */
    seed?: number;
}
/**
 * Integration with a chat model.
 */
export declare class ChatMistralAI<CallOptions extends MistralAICallOptions = MistralAICallOptions> extends BaseChatModel<CallOptions, AIMessageChunk> implements ChatMistralAIInput {
    static lc_name(): string;
    modelName: string;
    model: string;
    apiKey: string;
    endpoint?: string;
    temperature: number;
    streaming: boolean;
    topP: number;
    maxTokens: number;
    /**
     * @deprecated use safePrompt instead
     */
    safeMode: boolean;
    safePrompt: boolean;
    randomSeed?: number;
    seed?: number;
    lc_serializable: boolean;
    streamUsage: boolean;
    constructor(fields?: ChatMistralAIInput);
    getLsParams(options: this["ParsedCallOptions"]): LangSmithParams;
    _llmType(): string;
    /**
     * Get the parameters used to invoke the model
     */
    invocationParams(options?: this["ParsedCallOptions"]): Omit<ChatRequest, "messages">;
    bindTools(tools: (Record<string, unknown> | StructuredToolInterface | RunnableToolLike)[], kwargs?: Partial<CallOptions>): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions>;
    /**
     * Calls the MistralAI API with retry logic in case of failures.
     * @param {ChatRequest} input The input to send to the MistralAI API.
     * @returns {Promise<MistralAIChatCompletionResult | AsyncGenerator<MistralAIChatCompletionResult>>} The response from the MistralAI API.
     */
    completionWithRetry(input: ChatRequest, streaming: true): Promise<AsyncGenerator<ChatCompletionResponseChunk>>;
    completionWithRetry(input: ChatRequest, streaming: false): Promise<ChatCompletionResponse>;
    /** @ignore */
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    /** @ignore */
    _combineLLMOutput(): never[];
    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: StructuredOutputMethodParams<RunOutput, false> | z.ZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;
    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: StructuredOutputMethodParams<RunOutput, true> | z.ZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {
        raw: BaseMessage;
        parsed: RunOutput;
    }>;
    /** @ignore */
    private imports;
}
export {};
