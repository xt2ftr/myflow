import type { Anthropic } from "@anthropic-ai/sdk";
import { CustomEvent } from "@llamaindex/env";
import type { NodeWithScore } from "../Node.js";
import type { AgentEndEvent, AgentStartEvent } from "../agent/types.js";
import { EventCaller } from "../internal/context/EventCaller.js";
import type { LLMEndEvent, LLMStartEvent, LLMStreamEvent, LLMToolCallEvent, LLMToolResultEvent, RetrievalEndEvent, RetrievalStartEvent } from "../llm/types.js";
export declare class LlamaIndexCustomEvent<T = any> extends CustomEvent<T> {
    reason: EventCaller | null;
    private constructor();
    static fromEvent<Type extends keyof LlamaIndexEventMaps>(type: Type, detail: LlamaIndexEventMaps[Type]["detail"]): LlamaIndexCustomEvent<any>;
}
/**
 * This type is used to define the event maps.
 */
export interface LlamaIndexEventMaps {
    /**
     * @deprecated
     */
    retrieve: CustomEvent<RetrievalCallbackResponse>;
    "retrieve-start": RetrievalStartEvent;
    "retrieve-end": RetrievalEndEvent;
    /**
     * @deprecated
     */
    stream: CustomEvent<StreamCallbackResponse>;
    "llm-start": LLMStartEvent;
    "llm-end": LLMEndEvent;
    "llm-tool-call": LLMToolCallEvent;
    "llm-tool-result": LLMToolResultEvent;
    "llm-stream": LLMStreamEvent;
    "agent-start": AgentStartEvent;
    "agent-end": AgentEndEvent;
}
export interface DefaultStreamToken {
    id: string;
    object: string;
    created: number;
    model: string;
    choices: {
        index: number;
        delta: {
            content?: string | null;
            role?: "user" | "assistant" | "system" | "function" | "tool";
        };
        finish_reason: string | null;
    }[];
}
export type OpenAIStreamToken = DefaultStreamToken;
export type AnthropicStreamToken = Anthropic.Completion;
export interface StreamCallbackResponse {
    index: number;
    isDone?: boolean;
    token?: DefaultStreamToken;
}
export interface RetrievalCallbackResponse {
    query: string;
    nodes: NodeWithScore[];
}
interface CallbackManagerMethods {
    /**
     * onLLMStream is called when a token is streamed from the LLM. Defining this
     * callback auto sets the stream = True flag on the openAI createChatCompletion request.
     * @deprecated will be removed in the next major version
     */
    onLLMStream: (params: StreamCallbackResponse) => Promise<void> | void;
    /**
     * onRetrieve is called as soon as the retriever finishes fetching relevant nodes.
     * This callback allows you to handle the retrieved nodes even if the synthesizer
     * is still running.
     * @deprecated will be removed in the next major version
     */
    onRetrieve: (params: RetrievalCallbackResponse) => Promise<void> | void;
}
type EventHandler<Event extends CustomEvent> = (event: Event & {
    reason: EventCaller | null;
}) => void;
export declare class CallbackManager implements CallbackManagerMethods {
    #private;
    /**
     * @deprecated will be removed in the next major version
     */
    get onLLMStream(): CallbackManagerMethods["onLLMStream"];
    /**
     * @deprecated will be removed in the next major version
     */
    get onRetrieve(): CallbackManagerMethods["onRetrieve"];
    /**
     * @deprecated will be removed in the next major version
     */
    set onLLMStream(_: never);
    /**
     * @deprecated will be removed in the next major version
     */
    set onRetrieve(_: never);
    constructor(handlers?: Partial<CallbackManagerMethods>);
    on<K extends keyof LlamaIndexEventMaps, H extends EventHandler<LlamaIndexEventMaps[K]>>(event: K, handler: H): this;
    off<K extends keyof LlamaIndexEventMaps, H extends EventHandler<LlamaIndexEventMaps[K]>>(event: K, handler: H): this | undefined;
    dispatchEvent<K extends keyof LlamaIndexEventMaps>(event: K, detail: LlamaIndexEventMaps[K]["detail"]): void;
}
export {};
