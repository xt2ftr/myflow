import Replicate from "../internal/deps/replicate.js";
import { BaseLLM } from "./base.js";
import type { ChatMessage, ChatResponse, ChatResponseChunk, LLMChatParamsNonStreaming, LLMChatParamsStreaming, MessageType } from "./types.js";
export declare class ReplicateSession {
    replicateKey: string | null;
    replicate: Replicate;
    constructor(replicateKey?: string | null);
}
export declare const ALL_AVAILABLE_REPLICATE_MODELS: {
    "Llama-2-70b-chat-old": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-70b-chat-4bit": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-13b-chat-old": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-13b-chat-4bit": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-7b-chat-old": {
        contextWindow: number;
        replicateApi: string;
    };
    "Llama-2-7b-chat-4bit": {
        contextWindow: number;
        replicateApi: string;
    };
    "llama-3-70b-instruct": {
        contextWindow: number;
        replicateApi: string;
    };
    "llama-3-8b-instruct": {
        contextWindow: number;
        replicateApi: string;
    };
};
export declare enum ReplicateChatStrategy {
    A16Z = "a16z",
    META = "meta",
    METAWBOS = "metawbos",
    REPLICATE4BIT = "replicate4bit",
    REPLICATE4BITWNEWLINES = "replicate4bitwnewlines",
    LLAMA3 = "llama3"
}
export declare const DeuceChatStrategy: typeof ReplicateChatStrategy;
/**
 * Replicate LLM implementation used
 */
export declare class ReplicateLLM extends BaseLLM {
    model: keyof typeof ALL_AVAILABLE_REPLICATE_MODELS;
    chatStrategy: ReplicateChatStrategy;
    temperature: number;
    topP: number;
    maxTokens?: number;
    replicateSession: ReplicateSession;
    constructor(init?: Partial<ReplicateLLM> & {
        noWarn?: boolean;
    });
    get metadata(): {
        model: "Llama-2-70b-chat-old" | "Llama-2-70b-chat-4bit" | "Llama-2-13b-chat-old" | "Llama-2-13b-chat-4bit" | "Llama-2-7b-chat-old" | "Llama-2-7b-chat-4bit" | "llama-3-70b-instruct" | "llama-3-8b-instruct";
        temperature: number;
        topP: number;
        maxTokens: number | undefined;
        contextWindow: number;
        tokenizer: undefined;
    };
    mapMessagesToPrompt(messages: ChatMessage[]): {
        prompt: string;
        systemPrompt: import("./types.js").MessageContent | undefined;
    };
    mapMessagesToPromptLlama3(messages: ChatMessage[]): {
        prompt: string;
        systemPrompt: undefined;
    };
    mapMessagesToPromptA16Z(messages: ChatMessage[]): {
        prompt: string;
        systemPrompt: undefined;
    };
    mapMessageTypeA16Z(messageType: MessageType): string;
    mapMessagesToPromptMeta(messages: ChatMessage[], opts?: {
        withBos?: boolean;
        replicate4Bit?: boolean;
        withNewlines?: boolean;
    }): {
        prompt: string;
        systemPrompt: import("./types.js").MessageContent | undefined;
    };
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
}
export declare const LlamaDeuce: typeof ReplicateLLM;
