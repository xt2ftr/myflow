import { HfInference, type Options as HfInferenceOptions } from "@huggingface/inference";
import type { PreTrainedModel, PreTrainedTokenizer } from "@xenova/transformers";
import { BaseLLM } from "./base.js";
import type { ChatResponse, ChatResponseChunk, LLMChatParamsNonStreaming, LLMChatParamsStreaming, LLMMetadata } from "./types.js";
declare const DEFAULT_PARAMS: {
    temperature: number;
    topP: number;
    maxTokens: undefined;
    contextWindow: number;
};
export type HFConfig = Partial<typeof DEFAULT_PARAMS> & HfInferenceOptions & {
    model: string;
    accessToken: string;
    endpoint?: string;
};
/**
    Wrapper on the Hugging Face's Inference API.
    API Docs: https://huggingface.co/docs/huggingface.js/inference/README
    List of tasks with models: huggingface.co/api/tasks
    
    Note that Conversational API is not yet supported by the Inference API.
    They recommend using the text generation API instead.
    See: https://github.com/huggingface/huggingface.js/issues/586#issuecomment-2024059308
 */
export declare class HuggingFaceInferenceAPI extends BaseLLM {
    model: string;
    temperature: number;
    topP: number;
    maxTokens?: number;
    contextWindow: number;
    hf: HfInference;
    constructor(init: HFConfig);
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    private messagesToPrompt;
    protected nonStreamChat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected streamChat(params: LLMChatParamsStreaming): AsyncIterable<ChatResponseChunk>;
}
export interface HFLLMConfig {
    modelName?: string;
    tokenizerName?: string;
    temperature?: number;
    topP?: number;
    maxTokens?: number;
    contextWindow?: number;
}
export declare class HuggingFaceLLM extends BaseLLM {
    modelName: string;
    tokenizerName: string;
    temperature: number;
    topP: number;
    maxTokens?: number;
    contextWindow: number;
    private tokenizer;
    private model;
    constructor(init?: HFLLMConfig);
    get metadata(): LLMMetadata;
    getTokenizer(): Promise<PreTrainedTokenizer>;
    getModel(): Promise<PreTrainedModel>;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected nonStreamChat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected streamChat(params: LLMChatParamsStreaming): AsyncIterable<ChatResponseChunk>;
}
export {};
