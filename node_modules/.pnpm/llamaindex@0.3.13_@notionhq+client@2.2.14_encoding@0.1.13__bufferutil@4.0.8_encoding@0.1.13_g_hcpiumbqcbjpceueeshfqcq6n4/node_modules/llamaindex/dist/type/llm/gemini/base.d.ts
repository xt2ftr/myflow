import { GenerativeModel as GoogleGenerativeModel, type EnhancedGenerateContentResponse, type ModelParams as GoogleModelParams, type GenerateContentStreamResult as GoogleStreamGenerateContentResult } from "@google/generative-ai";
import { ToolCallLLM } from "../base.js";
import type { CompletionResponse, LLMCompletionParamsNonStreaming, LLMCompletionParamsStreaming, LLMMetadata } from "../types.js";
import { GEMINI_MODEL, type GeminiAdditionalChatOptions, type GeminiChatNonStreamResponse, type GeminiChatParamsNonStreaming, type GeminiChatParamsStreaming, type GeminiChatStreamResponse, type GeminiModelInfo, type GeminiSessionOptions, type GoogleGeminiSessionOptions, type IGeminiSession } from "./types.js";
export declare const GEMINI_MODEL_INFO_MAP: Record<GEMINI_MODEL, GeminiModelInfo>;
declare const DEFAULT_GEMINI_PARAMS: {
    model: GEMINI_MODEL;
    temperature: number;
    topP: number;
    maxTokens: undefined;
};
export type GeminiConfig = Partial<typeof DEFAULT_GEMINI_PARAMS> & {
    session?: IGeminiSession;
};
/**
 * Gemini Session to manage the connection to the Gemini API
 */
export declare class GeminiSession implements IGeminiSession {
    private gemini;
    constructor(options: GoogleGeminiSessionOptions);
    getGenerativeModel(metadata: GoogleModelParams): GoogleGenerativeModel;
    getResponseText(response: EnhancedGenerateContentResponse): string;
    getChatStream(result: GoogleStreamGenerateContentResult): GeminiChatStreamResponse;
    getCompletionStream(result: GoogleStreamGenerateContentResult): AsyncIterable<CompletionResponse>;
}
/**
 * Gemini Session Store to manage the current Gemini sessions
 */
export declare class GeminiSessionStore {
    static sessions: Array<{
        session: IGeminiSession;
        options: GeminiSessionOptions;
    }>;
    private static getSessionId;
    private static sessionMatched;
    static get(options?: GeminiSessionOptions): IGeminiSession;
}
/**
 * ToolCallLLM for Gemini
 */
export declare class Gemini extends ToolCallLLM<GeminiAdditionalChatOptions> {
    model: GEMINI_MODEL;
    temperature: number;
    topP: number;
    maxTokens?: number;
    session: IGeminiSession;
    constructor(init?: GeminiConfig);
    get supportToolCall(): boolean;
    get metadata(): LLMMetadata;
    protected nonStreamChat(params: GeminiChatParamsNonStreaming): Promise<GeminiChatNonStreamResponse>;
    protected streamChat(params: GeminiChatParamsStreaming): GeminiChatStreamResponse;
    chat(params: GeminiChatParamsStreaming): Promise<GeminiChatStreamResponse>;
    chat(params: GeminiChatParamsNonStreaming): Promise<GeminiChatNonStreamResponse>;
    complete(params: LLMCompletionParamsStreaming): Promise<AsyncIterable<CompletionResponse>>;
    complete(params: LLMCompletionParamsNonStreaming): Promise<CompletionResponse>;
}
export {};
