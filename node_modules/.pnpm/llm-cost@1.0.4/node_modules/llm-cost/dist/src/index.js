"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.estimateCost = exports.tokenizeAndEstimateCost = void 0;
const lite_1 = require("tiktoken/lite");
const load_1 = require("tiktoken/load");
const registry_json_1 = __importDefault(require("tiktoken/registry.json"));
const model_to_encoding_json_1 = __importDefault(require("tiktoken/model_to_encoding.json"));
const model_prices_and_context_window_json_1 = __importDefault(require("../model_prices_and_context_window.json"));
const cachedModel = {};
const initTikToken = (modelName) => __awaiter(void 0, void 0, void 0, function* () {
    if (!cachedModel[modelName]) {
        const fallback = "gpt-3.5-turbo";
        const gptModelName = modelName in model_to_encoding_json_1.default ? modelName : fallback;
        if (modelName in model_to_encoding_json_1.default) {
            console.info(`Initializing ${modelName} tokenizer`);
        }
        else {
            console.info(`Tokenizer for ${modelName} not defined, falling back to ${fallback} tokenizer. If you want to contribute a tokenizer for ${modelName}, please open an issue at https://github.com/rogeriochaves/llm-cost/issues/new`);
        }
        const registryInfo = registry_json_1.default[model_to_encoding_json_1.default[gptModelName]];
        const model = yield (0, load_1.load)(registryInfo);
        const encoder = new lite_1.Tiktoken(model.bpe_ranks, model.special_tokens, model.pat_str);
        cachedModel[modelName] = { model, encoder };
    }
    return cachedModel[modelName];
});
function countTokens(model, text) {
    return __awaiter(this, void 0, void 0, function* () {
        if (!text)
            return 0;
        const { encoder } = yield initTikToken(model);
        return encoder.encode(text).length;
    });
}
function tokenizeAndEstimateCost({ model, input, output, }) {
    return __awaiter(this, void 0, void 0, function* () {
        const inputTokens = (input && (yield countTokens(model, input))) || 0;
        const outputTokens = (output && (yield countTokens(model, output))) || 0;
        const cost = estimateCost({ model, inputTokens, outputTokens });
        return {
            inputTokens,
            outputTokens,
            cost,
        };
    });
}
exports.tokenizeAndEstimateCost = tokenizeAndEstimateCost;
function estimateCost({ model, inputTokens, outputTokens, }) {
    const modelPrice = model_prices_and_context_window_json_1.default[model];
    const { input_cost_per_token, output_cost_per_token } = modelPrice || {};
    return input_cost_per_token || output_cost_per_token
        ? (inputTokens || 0) * (input_cost_per_token !== null && input_cost_per_token !== void 0 ? input_cost_per_token : 0) +
            (outputTokens || 0) * (output_cost_per_token !== null && output_cost_per_token !== void 0 ? output_cost_per_token : 0)
        : undefined;
}
exports.estimateCost = estimateCost;
