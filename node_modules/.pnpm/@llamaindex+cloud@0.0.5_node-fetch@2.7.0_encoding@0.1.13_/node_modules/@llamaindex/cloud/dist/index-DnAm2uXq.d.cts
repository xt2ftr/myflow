/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkUpdateComponentOne = ChromaVectorStore | PineconeVectorStore | PgVectorStore | QdrantVectorStore | WeaviateVectorStore;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkUpdateComponent = Record<string, unknown> | DataSinkUpdateComponentOne;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         sinkType: PlatformApi.ConfigurableDataSinkNames.Chroma
 *     }
 */
interface DataSinkUpdate {
    /** The name of the data sink. */
    name?: string;
    sinkType: ConfigurableDataSinkNames;
    component?: DataSinkUpdateComponent;
}

type index$a_DataSinkUpdate = DataSinkUpdate;
type index$a_DataSinkUpdateComponent = DataSinkUpdateComponent;
type index$a_DataSinkUpdateComponentOne = DataSinkUpdateComponentOne;
declare namespace index$a {
  export type { index$a_DataSinkUpdate as DataSinkUpdate, index$a_DataSinkUpdateComponent as DataSinkUpdateComponent, index$a_DataSinkUpdateComponentOne as DataSinkUpdateComponentOne };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceUpdateComponentOne = ReaderConfig | unknown | TextNode | Document;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceUpdateComponent = Record<string, unknown> | DataSourceUpdateComponentOne | ExternallyStoredComponent;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         sourceType: PlatformApi.ConfigurableDataSourceNames.Discord
 *     }
 */
interface DataSourceUpdate {
    /** The name of the data source. */
    name?: string;
    sourceType: ConfigurableDataSourceNames;
    component?: DataSourceUpdateComponent;
}

type index$9_DataSourceUpdate = DataSourceUpdate;
type index$9_DataSourceUpdateComponent = DataSourceUpdateComponent;
type index$9_DataSourceUpdateComponentOne = DataSourceUpdateComponentOne;
declare namespace index$9 {
  export type { index$9_DataSourceUpdate as DataSourceUpdate, index$9_DataSourceUpdateComponent as DataSourceUpdateComponent, index$9_DataSourceUpdateComponentOne as DataSourceUpdateComponentOne };
}

declare namespace index$8 {
  export {  };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface ApiKeyCreate {
    name?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface ApiKeyUpdate {
    name?: string;
}

type index$7_ApiKeyCreate = ApiKeyCreate;
type index$7_ApiKeyUpdate = ApiKeyUpdate;
declare namespace index$7 {
  export type { index$7_ApiKeyCreate as ApiKeyCreate, index$7_ApiKeyUpdate as ApiKeyUpdate };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface ListProjectsApiProjectGetRequest {
    projectName?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {
 *         name: "string"
 *     }
 */
interface ProjectUpdate {
    name: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {
 *         name: "string"
 *     }
 */
interface EvalDatasetCreate {
    /** The name of the EvalDataset. */
    name: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         appName: "string",
 *         results: {
 *             "string": []
 *         }
 *     }
 */
interface LocalEvalSetCreate {
    /** The name of the app. */
    appName: string;
    /** The eval results. */
    results: Record<string, LocalEval[]>;
}

type index$6_EvalDatasetCreate = EvalDatasetCreate;
type index$6_ListProjectsApiProjectGetRequest = ListProjectsApiProjectGetRequest;
type index$6_LocalEvalSetCreate = LocalEvalSetCreate;
type index$6_ProjectUpdate = ProjectUpdate;
declare namespace index$6 {
  export type { index$6_EvalDatasetCreate as EvalDatasetCreate, index$6_ListProjectsApiProjectGetRequest as ListProjectsApiProjectGetRequest, index$6_LocalEvalSetCreate as LocalEvalSetCreate, index$6_ProjectUpdate as ProjectUpdate };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         projectName: "string",
 *         pipelineType: PlatformApi.PipelineType.Playground
 *     }
 */
interface SearchPipelinesApiPipelineGetRequest {
    projectName: string;
    pipelineName?: string;
    pipelineType?: PipelineType;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface GetPipelineForProjectApiPipelinePipelineIdGetRequest {
    withManagedIngestionStatus?: boolean;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         presetRetrievalParameters: {},
 *         evalParameters: {
 *             llmModel: PlatformApi.SupportedEvalLlmModelNames.Gpt35Turbo
 *         }
 *     }
 */
interface PipelineUpdate {
    configuredTransformations?: ConfiguredTransformationItem[];
    /** List of data source IDs. When provided instead of data_sources, the data sources will be looked up by ID. */
    dataSourceIds?: string[];
    /** List of data sources. When provided instead of data_source_ids, the data sources will be created. */
    dataSources?: DataSourceCreate[];
    /** List of data sink IDs. When provided instead of data_sinks, the data sinks will be looked up by ID. */
    dataSinkIds?: string[];
    /** List of data sinks. When provided instead of data_sink_ids, the data sinks will be created. */
    dataSinks?: DataSinkCreate[];
    /** List of raw file names to be used as pipeline inputs. */
    rawFileNames?: string[];
    /** Preset retrieval parameters for the pipeline. */
    presetRetrievalParameters?: PresetRetrievalParams;
    /** Eval parameters for the pipeline. */
    evalParameters?: EvalExecutionParams;
    /** Whether to use LlamaParse during pipeline execution. */
    llamaParseEnabled?: boolean;
    name?: string;
    /** The ID of the ManagedPipeline this playground pipeline is linked to. */
    managedPipelineId?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest {
    managedPipelineName?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest {
    playgroundPipelineName?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest {
    loadedFileIds?: string | string[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest {
    configuredTransformationId?: string;
    loadedFileId?: string;
    offset?: number;
    limit?: number;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * @example
 *     {
 *         evalQuestionIds: [],
 *         params: {
 *             llmModel: PlatformApi.SupportedEvalLlmModelNames.Gpt35Turbo
 *         }
 *     }
 */
interface EvalExecutionCreate {
    evalQuestionIds: string[];
    /** The parameters for the eval execution that will override the ones set in the pipeline. */
    params?: EvalExecutionParamsOverride;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {
 *         query: "string"
 *     }
 */
interface RetrievalParams {
    /** Number of nodes for dense retrieval. */
    denseSimilarityTopK?: number;
    /** Number of nodes for sparse retrieval. */
    sparseSimilarityTopK?: number;
    /** Enable reranking for retrieval */
    enableReranking?: boolean;
    /** Number of reranked nodes for returning. */
    rerankTopN?: number;
    /** Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval. */
    alpha?: number;
    /** Search filters for retrieval. the format of search_filters is a dict of {key: (operator, value)} */
    searchFilters?: Record<string, unknown[]>;
    /** The query to retrieve against. */
    query: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface BodyUpsertRawFileApiPipelinePipelineIdRawFilePut {
}

type index$5_BodyUpsertRawFileApiPipelinePipelineIdRawFilePut = BodyUpsertRawFileApiPipelinePipelineIdRawFilePut;
type index$5_CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest = CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest;
type index$5_CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest = CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest;
type index$5_DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest = DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest;
type index$5_EvalExecutionCreate = EvalExecutionCreate;
type index$5_GetPipelineForProjectApiPipelinePipelineIdGetRequest = GetPipelineForProjectApiPipelinePipelineIdGetRequest;
type index$5_GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest = GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest;
type index$5_PipelineUpdate = PipelineUpdate;
type index$5_RetrievalParams = RetrievalParams;
type index$5_SearchPipelinesApiPipelineGetRequest = SearchPipelinesApiPipelineGetRequest;
declare namespace index$5 {
  export type { index$5_BodyUpsertRawFileApiPipelinePipelineIdRawFilePut as BodyUpsertRawFileApiPipelinePipelineIdRawFilePut, index$5_CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest as CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest, index$5_CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest as CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest, index$5_DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest as DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest, index$5_EvalExecutionCreate as EvalExecutionCreate, index$5_GetPipelineForProjectApiPipelinePipelineIdGetRequest as GetPipelineForProjectApiPipelinePipelineIdGetRequest, index$5_GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest as GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest, index$5_PipelineUpdate as PipelineUpdate, index$5_RetrievalParams as RetrievalParams, index$5_SearchPipelinesApiPipelineGetRequest as SearchPipelinesApiPipelineGetRequest };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {
 *         name: "string"
 *     }
 */
interface EvalDatasetUpdate {
    /** The name of the EvalDataset. */
    name: string;
}

type index$4_EvalDatasetUpdate = EvalDatasetUpdate;
declare namespace index$4 {
  export type { index$4_EvalDatasetUpdate as EvalDatasetUpdate };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface BodyUploadFileApiParsingUploadPost {
    language: ParserLanguages[];
    parsingInstruction: string;
}

type index$3_BodyUploadFileApiParsingUploadPost = BodyUploadFileApiParsingUploadPost;
declare namespace index$3 {
  export type { index$3_BodyUploadFileApiParsingUploadPost as BodyUploadFileApiParsingUploadPost };
}

declare namespace index$2 {
  export {  };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {
 *         successUrl: "string",
 *         cancelUrl: "string"
 *     }
 */
interface CheckoutSessionCreatePayload {
    successUrl: string;
    cancelUrl: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {
 *         returnUrl: "string"
 *     }
 */
interface CustomerPortalSessionCreatePayload {
    returnUrl: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * @example
 *     {}
 */
interface StripeWebhookApiBillingWebhookPostRequest {
    stripeSignature?: string;
}

type index$1_CheckoutSessionCreatePayload = CheckoutSessionCreatePayload;
type index$1_CustomerPortalSessionCreatePayload = CustomerPortalSessionCreatePayload;
type index$1_StripeWebhookApiBillingWebhookPostRequest = StripeWebhookApiBillingWebhookPostRequest;
declare namespace index$1 {
  export type { index$1_CheckoutSessionCreatePayload as CheckoutSessionCreatePayload, index$1_CustomerPortalSessionCreatePayload as CustomerPortalSessionCreatePayload, index$1_StripeWebhookApiBillingWebhookPostRequest as StripeWebhookApiBillingWebhookPostRequest };
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for an API Key.
 */
interface ApiKey {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    name?: string;
    userId: string;
    redactedApiKey: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * OpenAI class for embeddings.
 *
 * Args:
 * mode (str): Mode for embedding.
 * Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.
 * Options are:
 *
 *         - OpenAIEmbeddingMode.SIMILARITY_MODE
 *         - OpenAIEmbeddingMode.TEXT_SEARCH_MODE
 *
 *     model (str): Model for embedding.
 *         Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.
 *         Options are:
 *
 *         - OpenAIEmbeddingModelType.DAVINCI
 *         - OpenAIEmbeddingModelType.CURIE
 *         - OpenAIEmbeddingModelType.BABBAGE
 *         - OpenAIEmbeddingModelType.ADA
 *         - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002
 */
interface AzureOpenAiEmbedding {
    /** The name of the embedding model. */
    modelName?: string;
    /** The batch size for embedding calls. */
    embedBatchSize?: number;
    callbackManager?: Record<string, unknown>;
    /** Additional kwargs for the OpenAI API. */
    additionalKwargs?: Record<string, unknown>;
    /** The OpenAI API key. */
    apiKey: string;
    /** The base URL for Azure deployment. */
    apiBase?: string;
    /** The version for Azure OpenAI API. */
    apiVersion?: string;
    /** Maximum number of retries. */
    maxRetries?: number;
    /** Timeout for each request. */
    timeout?: number;
    /** The default headers for API requests. */
    defaultHeaders?: Record<string, string>;
    /** Reuse the OpenAI client between requests. When doing anything with large volumes of async API calls, setting this to false can improve stability. */
    reuseClient?: boolean;
    /** The number of dimensions on the output embedding vectors. Works only with v3 embedding models. */
    dimensions?: number;
    /** The Azure endpoint to use. */
    azureEndpoint?: string;
    /** The Azure deployment to use. */
    azureDeployment?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Serialiable Data Loader with Pydantic.
 */
interface BasePydanticReader {
    /** Whether the data is loaded from a remote API or a local file. */
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * BeautifulSoup web page reader.
 *
 * Reads pages from the web.
 * Requires the `bs4` and `urllib` packages.
 *
 * Args:
 * website_extractor (Optional[Dict[str, Callable]]): A mapping of website
 * hostname (e.g. google.com) to a function that specifies how to
 * extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.
 */
interface BeautifulSoupWebReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Chat message.
 */
interface ChatMessage {
    role?: MessageRole;
    content?: unknown;
    additionalKwargs?: Record<string, unknown>;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Chroma vector store.
 *
 * In this vector store, embeddings are stored within a ChromaDB collection.
 *
 * During query time, the index uses ChromaDB to query for the top
 * k most similar nodes.
 *
 * Args:
 * chroma_collection (chromadb.api.models.Collection.Collection):
 * ChromaDB collection instance
 */
interface ChromaVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    flatMetadata?: boolean;
    collectionName?: string;
    host?: string;
    port?: string;
    ssl: boolean;
    headers?: Record<string, string>;
    persistDir?: string;
    collectionKwargs?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Split code using a AST parser.
 *
 * Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.
 * https://docs.sweep.dev/blogs/chunking-2m-files
 */
interface CodeSplitter {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    /** The programming language of the code being split. */
    language: string;
    /** The number of lines to include in each chunk. */
    chunkLines?: number;
    /** How many lines of code each chunk overlaps with. */
    chunkLinesOverlap?: number;
    /** Maximum number of characters per chunk. */
    maxChars?: number;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type ConfigurableDataSinkNames = "CHROMA" | "PINECONE" | "POSTGRES" | "QDRANT" | "WEAVIATE";
declare const ConfigurableDataSinkNames: {
    readonly Chroma: "CHROMA";
    readonly Pinecone: "PINECONE";
    readonly Postgres: "POSTGRES";
    readonly Qdrant: "QDRANT";
    readonly Weaviate: "WEAVIATE";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type ConfigurableDataSourceNames = "DISCORD" | "NOTION_PAGE" | "SLACK" | "SIMPLE_WEB_PAGE" | "TRAFILATURA_WEB_PAGE" | "BEAUTIFUL_SOUP_WEB_PAGE" | "RSS" | "YOUTUBE_TRANSCRIPT" | "GOOGLE_DOCS" | "GOOGLE_SHEETS" | "S3" | "READER" | "DOCUMENT_GROUP" | "TEXT_NODE" | "DOCUMENT";
declare const ConfigurableDataSourceNames: {
    readonly Discord: "DISCORD";
    readonly NotionPage: "NOTION_PAGE";
    readonly Slack: "SLACK";
    readonly SimpleWebPage: "SIMPLE_WEB_PAGE";
    readonly TrafilaturaWebPage: "TRAFILATURA_WEB_PAGE";
    readonly BeautifulSoupWebPage: "BEAUTIFUL_SOUP_WEB_PAGE";
    readonly Rss: "RSS";
    readonly YoutubeTranscript: "YOUTUBE_TRANSCRIPT";
    readonly GoogleDocs: "GOOGLE_DOCS";
    readonly GoogleSheets: "GOOGLE_SHEETS";
    readonly S3: "S3";
    readonly Reader: "READER";
    readonly DocumentGroup: "DOCUMENT_GROUP";
    readonly TextNode: "TEXT_NODE";
    readonly Document: "DOCUMENT";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a transformation definition.
 */
interface ConfigurableTransformationDefinition {
    /** The label field will be used to display the name of the component in the UI */
    label: string;
    /** The json_schema field can be used by clients to determine how to construct the component */
    jsonSchema: Record<string, unknown>;
    /** The name field will act as the unique identifier of TransformationDefinition objects */
    configurableTransformationType: ConfigurableTransformationNames;
    /** The transformation_category field will be used to group transformations in the UI */
    transformationCategory: TransformationCategoryNames;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type ConfigurableTransformationNames = "CODE_NODE_PARSER" | "SENTENCE_AWARE_NODE_PARSER" | "TOKEN_AWARE_NODE_PARSER" | "HTML_NODE_PARSER" | "MARKDOWN_NODE_PARSER" | "JSON_NODE_PARSER" | "SIMPLE_FILE_NODE_PARSER" | "OPENAI_EMBEDDING" | "AZURE_EMBEDDING";
declare const ConfigurableTransformationNames: {
    readonly CodeNodeParser: "CODE_NODE_PARSER";
    readonly SentenceAwareNodeParser: "SENTENCE_AWARE_NODE_PARSER";
    readonly TokenAwareNodeParser: "TOKEN_AWARE_NODE_PARSER";
    readonly HtmlNodeParser: "HTML_NODE_PARSER";
    readonly MarkdownNodeParser: "MARKDOWN_NODE_PARSER";
    readonly JsonNodeParser: "JSON_NODE_PARSER";
    readonly SimpleFileNodeParser: "SIMPLE_FILE_NODE_PARSER";
    readonly OpenaiEmbedding: "OPENAI_EMBEDDING";
    readonly AzureEmbedding: "AZURE_EMBEDDING";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type ConfiguredTransformationItemComponentOne = CodeSplitter | SentenceSplitter | TokenTextSplitter | HtmlNodeParser | MarkdownNodeParser | JsonNodeParser | SimpleFileNodeParser | OpenAiEmbedding | AzureOpenAiEmbedding;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Component that implements the transformation
 */
type ConfiguredTransformationItemComponent = Record<string, unknown> | ConfiguredTransformationItemComponentOne;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Configured transformations for pipelines.
 *
 * Similar to ConfigurableTransformation but includes a few
 * more fields that are useful to the platform.
 */
interface ConfiguredTransformationItem {
    id?: string;
    /** Name for the type of transformation this is (e.g. SIMPLE_NODE_PARSER). Can also be an enum instance of llama_index.ingestion.transformations.ConfigurableTransformations. This will be converted to ConfigurableTransformationNames. */
    configurableTransformationType: ConfigurableTransformationNames;
    /** Component that implements the transformation */
    component: ConfiguredTransformationItemComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkComponentOne = ChromaVectorStore | PineconeVectorStore | PgVectorStore | QdrantVectorStore | WeaviateVectorStore;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkComponent = Record<string, unknown> | DataSinkComponentOne;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a data sink.
 */
interface DataSink {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The name of the data sink. */
    name: string;
    sinkType: ConfigurableDataSinkNames;
    component: DataSinkComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkCreateComponentOne = ChromaVectorStore | PineconeVectorStore | PgVectorStore | QdrantVectorStore | WeaviateVectorStore;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSinkCreateComponent = Record<string, unknown> | DataSinkCreateComponentOne;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for creating a data sink.
 */
interface DataSinkCreate {
    /** The name of the data sink. */
    name: string;
    sinkType: ConfigurableDataSinkNames;
    component: DataSinkCreateComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a data sink definition.
 */
interface DataSinkDefinition {
    /** The label field will be used to display the name of the component in the UI */
    label: string;
    /** The json_schema field can be used by clients to determine how to construct the component */
    jsonSchema: Record<string, unknown>;
    /** The name field will act as the unique identifier of DataSinkDefinition objects */
    sinkType: ConfigurableDataSinkNames;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceComponentOne = ReaderConfig | unknown | TextNode | Document;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceComponent = Record<string, unknown> | DataSourceComponentOne | ExternallyStoredComponent;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a data source.
 */
interface DataSource {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The name of the data source. */
    name: string;
    sourceType: ConfigurableDataSourceNames;
    component: DataSourceComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceCreateComponentOne = ReaderConfig | unknown | TextNode | Document;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DataSourceCreateComponent = Record<string, unknown> | DataSourceCreateComponentOne | ExternallyStoredComponent;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for creating a data source.
 */
interface DataSourceCreate {
    /** The name of the data source. */
    name: string;
    sourceType: ConfigurableDataSourceNames;
    component: DataSourceCreateComponent;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a data source definition.
 */
interface DataSourceDefinition {
    /** The label field will be used to display the name of the component in the UI */
    label: string;
    /** The json_schema field can be used by clients to determine how to construct the component */
    jsonSchema: Record<string, unknown>;
    /** The name field will act as the unique identifier of DataSourceDefinition objects */
    sourceType: ConfigurableDataSourceNames;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that loads from a data source.
 */
interface DataSourceLoadJobRecord {
    /** Unique identifier */
    id?: string;
    jobName?: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that executes managed pipeline ingestion over a single data-source linked to a pipeline.
 */
interface DataSourceManagedIngestionJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Discord reader.
 *
 * Reads conversations from channels.
 *
 * Args:
 * discord_token (Optional[str]): Discord token. If not provided, we
 * assume the environment variable `DISCORD_TOKEN` is set.
 */
interface DiscordReader {
    isRemote?: boolean;
    discordToken: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type DocumentRelationshipsValue = RelatedNodeInfo | RelatedNodeInfo[];

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Generic interface for a data document.
 *
 * This document connects to data sources.
 */
interface Document {
    /** Unique ID of the node. */
    docId?: string;
    /** Embedding of the node. */
    embedding?: number[];
    /** A flat dictionary of metadata fields */
    extraInfo?: Record<string, unknown>;
    /** Metadata keys that are excluded from text for the embed model. */
    excludedEmbedMetadataKeys?: string[];
    /** Metadata keys that are excluded from text for the LLM. */
    excludedLlmMetadataKeys?: string[];
    /** A mapping of relationships to other node information. */
    relationships?: Record<string, DocumentRelationshipsValue>;
    /** Text content of the node. */
    text?: string;
    /** Start char index of the node. */
    startCharIdx?: number;
    /** End char index of the node. */
    endCharIdx?: number;
    /** Template for how text is formatted, with {content} and {metadata_str} placeholders. */
    textTemplate?: string;
    /** Template for how metadata is formatted, with {key} and {value} placeholders. */
    metadataTemplate?: string;
    /** Separator between metadata fields when converting to string. */
    metadataSeperator?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * A group of documents, usually separate pages from a single file.
 */
interface DocumentGroup {
    /** Whether the data is loaded from a remote API or a local file. */
    isRemote?: boolean;
    /** Path to the file containing the documents */
    filePath: string;
    /** Sequential group of documents, usually separate pages from a single file. */
    documents: Document[];
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for an eval dataset.
 * Includes the other DB fields like id, created_at, & updated_at.
 */
interface EvalDataset {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The name of the EvalDataset. */
    name: string;
    projectId: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that evaluates an EvalDataset against a pipeline.
 */
interface EvalDatasetJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
    /** The IDs for the EvalQuestions this execution ran against. */
    evalQuestionIds: string[];
    /** The parameters for the eval execution. */
    evalExecutionParams: EvalExecutionParams;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the params for an eval execution.
 */
interface EvalExecutionParams {
    /** The LLM model to use within eval execution. */
    llmModel?: SupportedEvalLlmModelNames;
    /** The template to use for the question answering prompt. */
    qaPromptTmpl?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the params override for an eval execution.
 */
interface EvalExecutionParamsOverride {
    /** The LLM model to use within eval execution. */
    llmModel?: SupportedEvalLlmModelNames;
    /** The template to use for the question answering prompt. */
    qaPromptTmpl?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for an eval LLM model.
 */
interface EvalLlmModelData {
    /** The name of the LLM model. */
    name: string;
    /** The description of the LLM model. */
    description: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Base schema model containing common database fields.
 */
interface EvalQuestion {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The content of the question. */
    content: string;
    evalDatasetId: string;
    /** The index at which this question is positioned relative to the other questions in the linked EvalDataset. Client is responsible for setting this correctly. */
    evalDatasetIndex: number;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface EvalQuestionCreate {
    /** The content of the question. */
    content: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the result of an eval question job.
 */
interface EvalQuestionResult {
    /** The ID of the question that was executed. */
    evalQuestionId: string;
    /** The ID of the pipeline that the question was executed against. */
    pipelineId: string;
    /** The nodes retrieved by the pipeline for the given question. */
    sourceNodes: TextNode[];
    /** The answer to the question. */
    answer: string;
    /** The eval metrics for the question. */
    evalMetrics: Record<string, MetricResult>;
    /** The ID of the EvalDatasetJobRecord that this result was generated from. */
    evalDatasetExecutionId: string;
    /** The EvalExecutionParams that were used when this result was generated. */
    evalDatasetExecutionParams: EvalExecutionParams;
    /** The timestamp when the eval finished. */
    evalFinishedAt: Date;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Base schema model for BaseComponent classes used in the platform.
 * Comes with special serialization logic for types used commonly in platform codebase.
 */
interface ExternallyStoredComponent {
    /** The ID of the externally stored component. */
    id: string;
    /** The extra path prefix for the externally stored component. */
    extraPathPrefix: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Google Docs reader.
 *
 * Reads a page from Google Docs
 */
interface GoogleDocsReader {
    isRemote?: boolean;
    /** If set the document will be split on the specified heading level. */
    splitOnHeadingLevel?: number;
    /** Include table of contents elements. */
    includeToc?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Google Sheets reader.
 *
 * Reads a sheet as TSV from Google Sheets
 */
interface GoogleSheetsReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * HTML node parser.
 *
 * Splits a document into Nodes using custom HTML splitting logic.
 *
 * Args:
 * include_metadata (bool): whether to include metadata in nodes
 * include_prev_next_rel (bool): whether to include prev/next relationships
 */
interface HtmlNodeParser {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    /** HTML tags to extract text from. */
    tags?: string[];
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface HttpValidationError {
    detail?: ValidationError[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * JSON node parser.
 *
 * Splits a document into Nodes using custom JSON splitting logic.
 *
 * Args:
 * include_metadata (bool): whether to include metadata in nodes
 * include_prev_next_rel (bool): whether to include prev/next relationships
 */
interface JsonNodeParser {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Enum for executable pipeline job names.
 */
type JobNames = "load_documents_job" | "load_raw_files_job" | "playground_job" | "eval_dataset_job" | "pipeline_managed_ingestion_job" | "data_source_managed_ingestion_job" | "raw_file_managed_ingestion_job";
declare const JobNames: {
    readonly LoadDocumentsJob: "load_documents_job";
    readonly LoadRawFilesJob: "load_raw_files_job";
    readonly PlaygroundJob: "playground_job";
    readonly EvalDatasetJob: "eval_dataset_job";
    readonly PipelineManagedIngestionJob: "pipeline_managed_ingestion_job";
    readonly DataSourceManagedIngestionJob: "data_source_managed_ingestion_job";
    readonly RawFileManagedIngestionJob: "raw_file_managed_ingestion_job";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type LlamaParseSupportedFileExtensions = ".pdf" | ".doc" | ".docx" | ".docm" | ".dot" | ".dotx" | ".dotm" | ".rtf" | ".wps" | ".wpd" | ".sxw" | ".stw" | ".sxg" | ".pages" | ".mw" | ".mcw" | ".uot" | ".uof" | ".uos" | ".uop" | ".ppt" | ".pptx" | ".pot" | ".pptm" | ".potx" | ".potm" | ".key" | ".odp" | ".odg" | ".otp" | ".fopd" | ".sxi" | ".sti" | ".epub";
declare const LlamaParseSupportedFileExtensions: {
    readonly Pdf: ".pdf";
    readonly Doc: ".doc";
    readonly Docx: ".docx";
    readonly Docm: ".docm";
    readonly Dot: ".dot";
    readonly Dotx: ".dotx";
    readonly Dotm: ".dotm";
    readonly Rtf: ".rtf";
    readonly Wps: ".wps";
    readonly Wpd: ".wpd";
    readonly Sxw: ".sxw";
    readonly Stw: ".stw";
    readonly Sxg: ".sxg";
    readonly Pages: ".pages";
    readonly Mw: ".mw";
    readonly Mcw: ".mcw";
    readonly Uot: ".uot";
    readonly Uof: ".uof";
    readonly Uos: ".uos";
    readonly Uop: ".uop";
    readonly Ppt: ".ppt";
    readonly Pptx: ".pptx";
    readonly Pot: ".pot";
    readonly Pptm: ".pptm";
    readonly Potx: ".potx";
    readonly Potm: ".potm";
    readonly Key: ".key";
    readonly Odp: ".odp";
    readonly Odg: ".odg";
    readonly Otp: ".otp";
    readonly Fopd: ".fopd";
    readonly Sxi: ".sxi";
    readonly Sti: ".sti";
    readonly Epub: ".epub";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for reading a loaded file.
 */
interface LoadedFile {
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    name: string;
    /** Size of the file in bytes */
    fileSize?: number;
    /** File type (e.g. PDF, DOCX, etc.) */
    fileType?: string;
    /** Number of pages in the file */
    numPages?: number;
    dataSourceId?: string;
    rawFileName?: string;
    pipelineId: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for loaded file contents that are stored in blob storage (S3).
 */
interface LoadedFilePayload {
    loadedFileId: string;
    documents?: Document[];
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Evaluation result, EvaluationResult from llama_index.
 *
 * Output of an BaseEvaluator.
 */
interface LocalEval {
    /** Query string */
    query?: string;
    /** Context strings */
    contexts?: string[];
    /** Response string */
    response?: string;
    /** Binary evaluation result (passing or not) */
    passing?: boolean;
    /** Feedback or reasoning for the response */
    feedback?: string;
    /** Score for the response */
    score?: number;
    /** Used only for pairwise and specifies whether it is from original order of presented answers or flipped order */
    pairwiseSource?: string;
    /** Whether the evaluation result is an invalid one. */
    invalidResult?: boolean;
    /** Reason for invalid evaluation. */
    invalidReason?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the result of a local evaluation.
 */
interface LocalEvalResults {
    /** The ID of the project. */
    projectId: string;
    /** The ID of the local eval result set. */
    evalSetId?: string;
    /** The name of the app. */
    appName: string;
    /** The name of the eval. */
    evalName: string;
    /** The eval results. */
    result: LocalEval;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface LocalEvalSets {
    /** The ID of the eval set. */
    evalSetId: string;
    /** The name of the app. */
    appName: string;
    /** The time of the upload. */
    uploadTime: Date;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Status of managed ingestion.
 */
type ManagedIngestionStatus = "NOT_STARTED" | "CREATING" | "IN_PROGRESS" | "SUCCESS" | "ERROR";
declare const ManagedIngestionStatus: {
    readonly NotStarted: "NOT_STARTED";
    readonly Creating: "CREATING";
    readonly InProgress: "IN_PROGRESS";
    readonly Success: "SUCCESS";
    readonly Error: "ERROR";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Markdown node parser.
 *
 * Splits a document into Nodes using custom Markdown splitting logic.
 *
 * Args:
 * include_metadata (bool): whether to include metadata in nodes
 * include_prev_next_rel (bool): whether to include prev/next relationships
 */
interface MarkdownNodeParser {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Message role.
 */
type MessageRole = "system" | "user" | "assistant" | "function" | "tool" | "chatbot" | "model";
declare const MessageRole: {
    readonly System: "system";
    readonly User: "user";
    readonly Assistant: "assistant";
    readonly Function: "function";
    readonly Tool: "tool";
    readonly Chatbot: "chatbot";
    readonly Model: "model";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface MetricResult {
    /** Whether the metric passed or not. */
    passing?: boolean;
    /** The score for the metric. */
    score?: number;
    /** The reasoning for the metric. */
    feedback?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Notion Page reader.
 *
 * Reads a set of Notion pages.
 *
 * Args:
 * integration_token (str): Notion integration token.
 */
interface NotionPageReader {
    isRemote?: boolean;
    token: string;
    headers: Record<string, string>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type ObjectType = "1" | "2" | "3" | "4";
declare const ObjectType: {
    readonly One: "1";
    readonly Two: "2";
    readonly Three: "3";
    readonly Four: "4";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * OpenAI class for embeddings.
 *
 * Args:
 * mode (str): Mode for embedding.
 * Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.
 * Options are:
 *
 *         - OpenAIEmbeddingMode.SIMILARITY_MODE
 *         - OpenAIEmbeddingMode.TEXT_SEARCH_MODE
 *
 *     model (str): Model for embedding.
 *         Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.
 *         Options are:
 *
 *         - OpenAIEmbeddingModelType.DAVINCI
 *         - OpenAIEmbeddingModelType.CURIE
 *         - OpenAIEmbeddingModelType.BABBAGE
 *         - OpenAIEmbeddingModelType.ADA
 *         - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002
 */
interface OpenAiEmbedding {
    /** The name of the embedding model. */
    modelName?: string;
    /** The batch size for embedding calls. */
    embedBatchSize?: number;
    callbackManager?: Record<string, unknown>;
    /** Additional kwargs for the OpenAI API. */
    additionalKwargs?: Record<string, unknown>;
    /** The OpenAI API key. */
    apiKey: string;
    /** The base URL for OpenAI API. */
    apiBase?: string;
    /** The version for OpenAI API. */
    apiVersion?: string;
    /** Maximum number of retries. */
    maxRetries?: number;
    /** Timeout for each request. */
    timeout?: number;
    /** The default headers for API requests. */
    defaultHeaders?: Record<string, string>;
    /** Reuse the OpenAI client between requests. When doing anything with large volumes of async API calls, setting this to false can improve stability. */
    reuseClient?: boolean;
    /** The number of dimensions on the output embedding vectors. Works only with v3 embedding models. */
    dimensions?: number;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Abstract vector store protocol.
 */
interface PgVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    connectionString: string;
    asyncConnectionString: string;
    tableName: string;
    schemaName: string;
    embedDim: number;
    hybridSearch: boolean;
    textSearchConfig: string;
    cacheOk: boolean;
    performSetup: boolean;
    debug: boolean;
    useJsonb: boolean;
    flatMetadata?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Enum for representing the languages supported by the parser
 */
type ParserLanguages = "af" | "az" | "bs" | "cs" | "cy" | "da" | "de" | "en" | "es" | "et" | "fr" | "ga" | "hr" | "hu" | "id" | "is" | "it" | "ku" | "la" | "lt" | "lv" | "mi" | "ms" | "mt" | "nl" | "no" | "oc" | "pi" | "pl" | "pt" | "ro" | "rs_latin" | "sk" | "sl" | "sq" | "sv" | "sw" | "tl" | "tr" | "uz" | "vi" | "ar" | "fa" | "ug" | "ur" | "bn" | "as" | "mni" | "ru" | "rs_cyrillic" | "be" | "bg" | "uk" | "mn" | "abq" | "ady" | "kbd" | "ava" | "dar" | "inh" | "che" | "lbe" | "lez" | "tab" | "tjk" | "hi" | "mr" | "ne" | "bh" | "mai" | "ang" | "bho" | "mah" | "sck" | "new" | "gom" | "sa" | "bgc" | "th" | "ch_sim" | "ch_tra" | "ja" | "ko" | "ta" | "te" | "kn";
declare const ParserLanguages: {
    readonly Af: "af";
    readonly Az: "az";
    readonly Bs: "bs";
    readonly Cs: "cs";
    readonly Cy: "cy";
    readonly Da: "da";
    readonly De: "de";
    readonly En: "en";
    readonly Es: "es";
    readonly Et: "et";
    readonly Fr: "fr";
    readonly Ga: "ga";
    readonly Hr: "hr";
    readonly Hu: "hu";
    readonly Id: "id";
    readonly Is: "is";
    readonly It: "it";
    readonly Ku: "ku";
    readonly La: "la";
    readonly Lt: "lt";
    readonly Lv: "lv";
    readonly Mi: "mi";
    readonly Ms: "ms";
    readonly Mt: "mt";
    readonly Nl: "nl";
    readonly No: "no";
    readonly Oc: "oc";
    readonly Pi: "pi";
    readonly Pl: "pl";
    readonly Pt: "pt";
    readonly Ro: "ro";
    readonly RsLatin: "rs_latin";
    readonly Sk: "sk";
    readonly Sl: "sl";
    readonly Sq: "sq";
    readonly Sv: "sv";
    readonly Sw: "sw";
    readonly Tl: "tl";
    readonly Tr: "tr";
    readonly Uz: "uz";
    readonly Vi: "vi";
    readonly Ar: "ar";
    readonly Fa: "fa";
    readonly Ug: "ug";
    readonly Ur: "ur";
    readonly Bn: "bn";
    readonly As: "as";
    readonly Mni: "mni";
    readonly Ru: "ru";
    readonly RsCyrillic: "rs_cyrillic";
    readonly Be: "be";
    readonly Bg: "bg";
    readonly Uk: "uk";
    readonly Mn: "mn";
    readonly Abq: "abq";
    readonly Ady: "ady";
    readonly Kbd: "kbd";
    readonly Ava: "ava";
    readonly Dar: "dar";
    readonly Inh: "inh";
    readonly Che: "che";
    readonly Lbe: "lbe";
    readonly Lez: "lez";
    readonly Tab: "tab";
    readonly Tjk: "tjk";
    readonly Hi: "hi";
    readonly Mr: "mr";
    readonly Ne: "ne";
    readonly Bh: "bh";
    readonly Mai: "mai";
    readonly Ang: "ang";
    readonly Bho: "bho";
    readonly Mah: "mah";
    readonly Sck: "sck";
    readonly New: "new";
    readonly Gom: "gom";
    readonly Sa: "sa";
    readonly Bgc: "bgc";
    readonly Th: "th";
    readonly ChSim: "ch_sim";
    readonly ChTra: "ch_tra";
    readonly Ja: "ja";
    readonly Ko: "ko";
    readonly Ta: "ta";
    readonly Te: "te";
    readonly Kn: "kn";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface ParsingJob {
    id: string;
    status: StatusEnum;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ParsingJobMarkdownResult {
    /** The markdown result of the parsing job */
    markdown: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ParsingJobTextResult {
    /** The text result of the parsing job */
    text: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
interface ParsingUsage {
    usagePdfPages: number;
    maxPdfPages: number;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Pinecone Vector Store.
 *
 * In this vector store, embeddings and docs are stored within a
 * Pinecone index.
 *
 * During query time, the index uses Pinecone to query for the top
 * k most similar nodes.
 *
 * Args:
 * pinecone_index (Optional[Union[pinecone.Pinecone.Index, pinecone.Index]]): Pinecone index instance,
 * pinecone.Pinecone.Index for clients >= 3.0.0; pinecone.Index for older clients.
 * insert_kwargs (Optional[Dict]): insert kwargs during `upsert` call.
 * add_sparse_vector (bool): whether to add sparse vector to index.
 * tokenizer (Optional[Callable]): tokenizer to use to generate sparse
 * default_empty_query_vector (Optional[List[float]]): default empty query vector.
 * Defaults to None. If not None, then this vector will be used as the query
 * vector if the query is empty.
 */
interface PineconeVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    flatMetadata?: boolean;
    apiKey?: string;
    indexName?: string;
    environment?: string;
    namespace?: string;
    insertKwargs?: Record<string, unknown>;
    addSparseVector: boolean;
    textKey: string;
    batchSize: number;
    removeTextFromMetadata: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a pipeline.
 */
interface Pipeline {
    configuredTransformations: ConfiguredTransformationItem[];
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    name: string;
    projectId: string;
    /** Type of pipeline. Either PLAYGROUND or MANAGED. */
    pipelineType?: PipelineType;
    /** The ID of the ManagedPipeline this playground pipeline is linked to. */
    managedPipelineId?: string;
    /** Preset retrieval parameters for the pipeline. */
    presetRetrievalParameters?: PresetRetrievalParams;
    /** Eval parameters for the pipeline. */
    evalParameters?: EvalExecutionParams;
    /** Whether to use LlamaParse during pipeline execution. */
    llamaParseEnabled?: boolean;
    /** Status of Managed Ingestion. */
    managedIngestionStatus?: ManagedIngestionStatus;
    dataSources: DataSource[];
    dataSinks: DataSink[];
    /** List of raw file names to be used as pipeline inputs. */
    rawFileNames?: string[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for creating a pipeline.
 */
interface PipelineCreate {
    /** List of configured transformations. */
    configuredTransformations?: ConfiguredTransformationItem[];
    /** List of data source IDs. When provided instead of data_sources, the data sources will be looked up by ID. */
    dataSourceIds?: string[];
    /** List of data sources. When provided instead of data_source_ids, the data sources will be created. */
    dataSources?: DataSourceCreate[];
    /** List of data sink IDs. When provided instead of data_sinks, the data sinks will be looked up by ID. */
    dataSinkIds?: string[];
    /** List of data sinks. When provided instead of data_sink_ids, the data sinks will be created. */
    dataSinks?: DataSinkCreate[];
    /** List of raw file names to be used as pipeline inputs. */
    rawFileNames?: string[];
    /** Preset retrieval parameters for the pipeline. */
    presetRetrievalParameters?: PresetRetrievalParams;
    /** Eval parameters for the pipeline. */
    evalParameters?: EvalExecutionParams;
    /** Whether to use LlamaParse during pipeline execution. */
    llamaParseEnabled?: boolean;
    name: string;
    /** Type of pipeline. Either PLAYGROUND or MANAGED. */
    pipelineType?: PipelineType;
    /** The ID of the ManagedPipeline this playground pipeline is linked to. */
    managedPipelineId?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that executes managed pipeline ingestion over all data-sources linked to a pipeline.
 */
interface PipelineManagedIngestionJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Enum for representing the type of a pipeline
 */
type PipelineType = "PLAYGROUND" | "MANAGED";
declare const PipelineType: {
    readonly Playground: "PLAYGROUND";
    readonly Managed: "MANAGED";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type PlatformTextNodeRelationshipsValue = RelatedNodeInfo | RelatedNodeInfo[];

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Base node Object.
 *
 * Generic abstract interface for retrievable nodes
 */
interface PlatformTextNode {
    /** Unique ID of the node. */
    id?: string;
    /** Embedding of the node. */
    embedding?: number[];
    /** A flat dictionary of metadata fields */
    extraInfo?: Record<string, unknown>;
    /** Metadata keys that are excluded from text for the embed model. */
    excludedEmbedMetadataKeys?: string[];
    /** Metadata keys that are excluded from text for the LLM. */
    excludedLlmMetadataKeys?: string[];
    /** A mapping of relationships to other node information. */
    relationships?: Record<string, PlatformTextNodeRelationshipsValue>;
    /** Text content of the node. */
    text?: string;
    /** Start char index of the node. */
    startCharIdx?: number;
    /** End char index of the node. */
    endCharIdx?: number;
    /** Template for how text is formatted, with {content} and {metadata_str} placeholders. */
    textTemplate?: string;
    /** Template for how metadata is formatted, with {key} and {value} placeholders. */
    metadataTemplate?: string;
    /** Separator between metadata fields when converting to string. */
    metadataSeperator?: string;
    /** The pipeline id this node was generated from */
    pipelineId: string;
    /** The configured transformation id within a pipeline this node was generated from */
    configuredTransformationId: string;
    /** The time this node was created at */
    createdAt?: Date;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a job that executes a playground pipeline.
 */
interface PlaygroundJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
    /** The IDs for the LoadedFiles this execution ran against. */
    loadedFileIds?: string[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for the search params for an retrieval execution that can be preset for a pipeline.
 */
interface PresetRetrievalParams {
    /** Number of nodes for dense retrieval. */
    denseSimilarityTopK?: number;
    /** Number of nodes for sparse retrieval. */
    sparseSimilarityTopK?: number;
    /** Enable reranking for retrieval */
    enableReranking?: boolean;
    /** Number of reranked nodes for returning. */
    rerankTopN?: number;
    /** Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval. */
    alpha?: number;
    /** Search filters for retrieval. the format of search_filters is a dict of {key: (operator, value)} */
    searchFilters?: Record<string, unknown[]>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for a project.
 */
interface Project {
    name: string;
    /** Unique identifier */
    id: string;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    pipelines: Pipeline[];
    adHocEvalDatasetId?: string;
    /** The user ID of the project owner. */
    userId: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for creating a project.
 */
interface ProjectCreate {
    name: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the prompts derived from the PromptMixin.
 */
interface PromptMixinPrompts {
    /** The ID of the project. */
    projectId: string;
    /** The ID of the prompt set. */
    id?: string;
    /** The name of the prompt set. */
    name: string;
    /** The prompts. */
    prompts: PromptSpec[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface PromptSpec {
    /** The key of the prompt in the PromptMixin. */
    promptKey: string;
    /** The class of the prompt (PromptTemplate or ChatPromptTemplate). */
    promptClass: string;
    /** The type of prompt. */
    promptType: string;
    /** The template of the prompt. */
    template?: string;
    /** The chat message templates of the prompt. */
    messageTemplates?: ChatMessage[];
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Qdrant Vector Store.
 *
 * In this vector store, embeddings and docs are stored within a
 * Qdrant collection.
 *
 * During query time, the index uses Qdrant to query for the top
 * k most similar nodes.
 *
 * Args:
 * collection_name: (str): name of the Qdrant collection
 * client (Optional[Any]): QdrantClient instance from `qdrant-client` package
 * aclient (Optional[Any]): AsyncQdrantClient instance from `qdrant-client` package
 * url (Optional[str]): url of the Qdrant instance
 * api_key (Optional[str]): API key for authenticating with Qdrant
 * batch_size (int): number of points to upload in a single request to Qdrant. Defaults to 64
 * parallel (int): number of parallel processes to use during upload. Defaults to 1
 * max_retries (int): maximum number of retries in case of a failure. Defaults to 3
 * client_kwargs (Optional[dict]): additional kwargs for QdrantClient and AsyncQdrantClient
 * enable_hybrid (bool): whether to enable hybrid search using dense and sparse vectors
 * sparse_doc_fn (Optional[SparseEncoderCallable]): function to encode sparse vectors
 * sparse_query_fn (Optional[SparseEncoderCallable]): function to encode sparse queries
 * hybrid_fusion_fn (Optional[HybridFusionCallable]): function to fuse hybrid search results
 */
interface QdrantVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    flatMetadata?: boolean;
    collectionName: string;
    path?: string;
    url?: string;
    apiKey?: string;
    batchSize: number;
    parallel: number;
    maxRetries: number;
    clientKwargs?: Record<string, unknown>;
    enableHybrid: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Schema for creating a raw file.
 */
interface RawFile {
    name: string;
    /** Size of the file in bytes */
    fileSize: number;
    /** File type (e.g. pdf, docx, etc.) */
    fileType: string;
    /** The last modified time of the file */
    lastModifiedAt: Date;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for job that executes managed pipeline ingestion a Raw File linked to a pipeline.
 */
interface RawFileManagedIngestionJobRecord {
    /** Unique identifier */
    id?: string;
    jobName: JobNames;
    status: StatusEnum;
    startedAt?: Date;
    endedAt?: Date;
    /** Creation datetime */
    createdAt?: Date;
    /** Update datetime */
    updatedAt?: Date;
    /** The partitions for this execution. */
    partitions: Record<string, string>;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Represents a reader and it's input arguments.
 */
interface ReaderConfig {
    reader?: unknown;
    /** Reader args. */
    readerArgs?: unknown[];
    /** Reader kwargs. */
    readerKwargs?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Base component object to capture class names.
 */
interface RelatedNodeInfo {
    nodeId: string;
    nodeType?: ObjectType;
    metadata?: Record<string, unknown>;
    hash?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Schema for the result of an retrieval execution.
 */
interface RetrieveResults {
    /** The ID of the pipeline that the query was retrieved against. */
    pipelineId: string;
    /** The nodes retrieved by the pipeline for the given query. */
    retrievalNodes: TextNodeWithScore[];
    /** The end-to-end latency for retrieval and reranking. */
    retrievalLatency: Record<string, number>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * RSS reader.
 *
 * Reads content from an RSS feed.
 */
interface RssReader {
    isRemote?: boolean;
    htmlToText?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
type S3ReaderFileExtractorValue = string | unknown;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * General reader for any S3 file or directory.
 *
 * If key is not set, the entire bucket (filtered by prefix) is parsed.
 *
 * Args:
 * bucket (str): the name of your S3 bucket
 * key (Optional[str]): the name of the specific file. If none is provided,
 * this loader will iterate through the entire bucket.
 * prefix (Optional[str]): the prefix to filter by in the case that the loader
 * iterates through the entire bucket. Defaults to empty string.
 * recursive (bool): Whether to recursively search in subdirectories.
 * True by default.
 * file_extractor (Optional[Dict[str, BaseReader]]): A mapping of file
 * extension to a BaseReader class that specifies how to convert that file
 * to text. See `SimpleDirectoryReader` for more details.
 * required_exts (Optional[List[str]]): List of required extensions.
 * Default is None.
 * num_files_limit (Optional[int]): Maximum number of files to read.
 * Default is None.
 * file_metadata (Optional[Callable[str, Dict]]): A function that takes
 * in a filename and returns a Dict of metadata for the Document.
 * Default is None.
 * aws_access_id (Optional[str]): provide AWS access key directly.
 * aws_access_secret (Optional[str]): provide AWS access key directly.
 * s3_endpoint_url (Optional[str]): provide S3 endpoint URL directly.
 */
interface S3Reader {
    isRemote?: boolean;
    bucket: string;
    key?: string;
    prefix?: string;
    recursive?: boolean;
    fileExtractor?: Record<string, S3ReaderFileExtractorValue>;
    requiredExts?: string[];
    filenameAsId?: boolean;
    numFilesLimit?: number;
    awsAccessId?: string;
    awsAccessSecret?: string;
    awsSessionToken?: string;
    s3EndpointUrl?: string;
    customReaderPath?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Parse text with a preference for complete sentences.
 *
 * In general, this class tries to keep sentences and paragraphs together. Therefore
 * compared to the original TokenTextSplitter, there are less likely to be
 * hanging sentences or parts of sentences at the end of the node chunk.
 */
interface SentenceSplitter {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    /** The token chunk size for each chunk. */
    chunkSize?: number;
    /** The token overlap of each chunk when splitting. */
    chunkOverlap?: number;
    /** Default separator for splitting into words */
    separator?: string;
    /** Separator between paragraphs. */
    paragraphSeparator?: string;
    /** Backup regex for splitting into sentences. */
    secondaryChunkingRegex?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Simple file node parser.
 *
 * Splits a document loaded from a file into Nodes using logic based on the file type
 * automatically detects the NodeParser to use based on file type
 *
 * Args:
 * include_metadata (bool): whether to include metadata in nodes
 * include_prev_next_rel (bool): whether to include prev/next relationships
 */
interface SimpleFileNodeParser {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Simple web page reader.
 *
 * Reads pages from the web.
 *
 * Args:
 * html_to_text (bool): Whether to convert HTML to text.
 * Requires `html2text` package.
 * metadata_fn (Optional[Callable[[str], Dict]]): A function that takes in
 * a URL and returns a dictionary of metadata.
 * Default is None.
 */
interface SimpleWebPageReader {
    isRemote?: boolean;
    htmlToText: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Slack reader.
 *
 * Reads conversations from channels. If an earliest_date is provided, an
 * optional latest_date can also be provided. If no latest_date is provided,
 * we assume the latest date is the current timestamp.
 *
 * Args:
 * slack_token (Optional[str]): Slack token. If not provided, we
 * assume the environment variable `SLACK_BOT_TOKEN` is set.
 * ssl (Optional[str]): Custom SSL context. If not provided, it is assumed
 * there is already an SSL context available.
 * earliest_date (Optional[datetime]): Earliest date from which
 * to read conversations. If not provided, we read all messages.
 * latest_date (Optional[datetime]): Latest date from which to
 * read conversations. If not provided, defaults to current timestamp
 * in combination with earliest_date.
 */
interface SlackReader {
    isRemote?: boolean;
    slackToken: string;
    earliestDateTimestamp?: number;
    latestDateTimestamp: number;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Enum for representing the status of a job
 */
type StatusEnum = "PENDING" | "SUCCESS" | "ERROR" | "CANCELED";
declare const StatusEnum: {
    readonly Pending: "PENDING";
    readonly Success: "SUCCESS";
    readonly Error: "ERROR";
    readonly Canceled: "CANCELED";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Response Schema for a supported eval LLM model.
 */
interface SupportedEvalLlmModel {
    /** The name of the supported eval LLM model. */
    name: SupportedEvalLlmModelNames;
    /** The details of the supported eval LLM model. */
    details: EvalLlmModelData;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type SupportedEvalLlmModelNames = "GPT_3_5_TURBO" | "GPT_4" | "GPT_4_TURBO";
declare const SupportedEvalLlmModelNames: {
    readonly Gpt35Turbo: "GPT_3_5_TURBO";
    readonly Gpt4: "GPT_4";
    readonly Gpt4Turbo: "GPT_4_TURBO";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */

type TextNodeRelationshipsValue = RelatedNodeInfo | RelatedNodeInfo[];

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Base node Object.
 *
 * Generic abstract interface for retrievable nodes
 */
interface TextNode {
    /** Unique ID of the node. */
    id?: string;
    /** Embedding of the node. */
    embedding?: number[];
    /** A flat dictionary of metadata fields */
    extraInfo?: Record<string, unknown>;
    /** Metadata keys that are excluded from text for the embed model. */
    excludedEmbedMetadataKeys?: string[];
    /** Metadata keys that are excluded from text for the LLM. */
    excludedLlmMetadataKeys?: string[];
    /** A mapping of relationships to other node information. */
    relationships?: Record<string, TextNodeRelationshipsValue>;
    /** Text content of the node. */
    text?: string;
    /** Start char index of the node. */
    startCharIdx?: number;
    /** End char index of the node. */
    endCharIdx?: number;
    /** Template for how text is formatted, with {content} and {metadata_str} placeholders. */
    textTemplate?: string;
    /** Template for how metadata is formatted, with {key} and {value} placeholders. */
    metadataTemplate?: string;
    /** Separator between metadata fields when converting to string. */
    metadataSeperator?: string;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

/**
 * Same as NodeWithScore but type for node is a TextNode instead of BaseNode.
 * FastAPI doesn't accept abstract classes like BaseNode.
 */
interface TextNodeWithScore {
    node: TextNode;
    score?: number;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Implementation of splitting text that looks at word tokens.
 */
interface TokenTextSplitter {
    /** Whether or not to consider metadata when splitting. */
    includeMetadata?: boolean;
    /** Include prev/next node relationships. */
    includePrevNextRel?: boolean;
    callbackManager?: Record<string, unknown>;
    /** The token chunk size for each chunk. */
    chunkSize?: number;
    /** The token overlap of each chunk when splitting. */
    chunkOverlap?: number;
    /** Default separator for splitting into words */
    separator?: string;
    /** Additional separators for splitting. */
    backupSeparators?: unknown[];
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Trafilatura web page reader.
 *
 * Reads pages from the web.
 * Requires the `trafilatura` package.
 */
interface TrafilaturaWebReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * An enumeration.
 */
type TransformationCategoryNames = "NODE_PARSER" | "EMBEDDING";
declare const TransformationCategoryNames: {
    readonly NodeParser: "NODE_PARSER";
    readonly Embedding: "EMBEDDING";
};

/**
 * This file was auto-generated by Fern from our API Definition.
 */
type ValidationErrorLocItem = string | number;

/**
 * This file was auto-generated by Fern from our API Definition.
 */

interface ValidationError {
    loc: ValidationErrorLocItem[];
    msg: string;
    type: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Weaviate vector store.
 *
 * In this vector store, embeddings and docs are stored within a
 * Weaviate collection.
 *
 * During query time, the index uses Weaviate to query for the top
 * k most similar nodes.
 *
 * Args:
 * weaviate_client (weaviate.Client): WeaviateClient
 * instance from `weaviate-client` package
 * index_name (Optional[str]): name for Weaviate classes
 */
interface WeaviateVectorStore {
    storesText?: boolean;
    isEmbeddingQuery?: boolean;
    indexName: string;
    url?: string;
    textKey: string;
    authConfig?: Record<string, unknown>;
    clientKwargs?: Record<string, unknown>;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
/**
 * Youtube Transcript reader.
 */
interface YoutubeTranscriptReader {
    isRemote?: boolean;
    className?: string;
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */
declare class PlatformApiError extends Error {
    readonly statusCode?: number;
    readonly body?: unknown;
    constructor({ message, statusCode, body }: {
        message?: string;
        statusCode?: number;
        body?: unknown;
    });
}

/**
 * This file was auto-generated by Fern from our API Definition.
 */

declare class UnprocessableEntityError extends PlatformApiError {
    constructor(body: HttpValidationError);
}

type index_ApiKey = ApiKey;
type index_ApiKeyCreate = ApiKeyCreate;
type index_ApiKeyUpdate = ApiKeyUpdate;
type index_AzureOpenAiEmbedding = AzureOpenAiEmbedding;
type index_BasePydanticReader = BasePydanticReader;
type index_BeautifulSoupWebReader = BeautifulSoupWebReader;
type index_BodyUploadFileApiParsingUploadPost = BodyUploadFileApiParsingUploadPost;
type index_BodyUpsertRawFileApiPipelinePipelineIdRawFilePut = BodyUpsertRawFileApiPipelinePipelineIdRawFilePut;
type index_ChatMessage = ChatMessage;
type index_CheckoutSessionCreatePayload = CheckoutSessionCreatePayload;
type index_ChromaVectorStore = ChromaVectorStore;
type index_CodeSplitter = CodeSplitter;
declare const index_ConfigurableDataSinkNames: typeof ConfigurableDataSinkNames;
declare const index_ConfigurableDataSourceNames: typeof ConfigurableDataSourceNames;
type index_ConfigurableTransformationDefinition = ConfigurableTransformationDefinition;
declare const index_ConfigurableTransformationNames: typeof ConfigurableTransformationNames;
type index_ConfiguredTransformationItem = ConfiguredTransformationItem;
type index_ConfiguredTransformationItemComponent = ConfiguredTransformationItemComponent;
type index_ConfiguredTransformationItemComponentOne = ConfiguredTransformationItemComponentOne;
type index_CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest = CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest;
type index_CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest = CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest;
type index_CustomerPortalSessionCreatePayload = CustomerPortalSessionCreatePayload;
type index_DataSink = DataSink;
type index_DataSinkComponent = DataSinkComponent;
type index_DataSinkComponentOne = DataSinkComponentOne;
type index_DataSinkCreate = DataSinkCreate;
type index_DataSinkCreateComponent = DataSinkCreateComponent;
type index_DataSinkCreateComponentOne = DataSinkCreateComponentOne;
type index_DataSinkDefinition = DataSinkDefinition;
type index_DataSinkUpdate = DataSinkUpdate;
type index_DataSinkUpdateComponent = DataSinkUpdateComponent;
type index_DataSinkUpdateComponentOne = DataSinkUpdateComponentOne;
type index_DataSource = DataSource;
type index_DataSourceComponent = DataSourceComponent;
type index_DataSourceComponentOne = DataSourceComponentOne;
type index_DataSourceCreate = DataSourceCreate;
type index_DataSourceCreateComponent = DataSourceCreateComponent;
type index_DataSourceCreateComponentOne = DataSourceCreateComponentOne;
type index_DataSourceDefinition = DataSourceDefinition;
type index_DataSourceLoadJobRecord = DataSourceLoadJobRecord;
type index_DataSourceManagedIngestionJobRecord = DataSourceManagedIngestionJobRecord;
type index_DataSourceUpdate = DataSourceUpdate;
type index_DataSourceUpdateComponent = DataSourceUpdateComponent;
type index_DataSourceUpdateComponentOne = DataSourceUpdateComponentOne;
type index_DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest = DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest;
type index_DiscordReader = DiscordReader;
type index_Document = Document;
type index_DocumentGroup = DocumentGroup;
type index_DocumentRelationshipsValue = DocumentRelationshipsValue;
type index_EvalDataset = EvalDataset;
type index_EvalDatasetCreate = EvalDatasetCreate;
type index_EvalDatasetJobRecord = EvalDatasetJobRecord;
type index_EvalDatasetUpdate = EvalDatasetUpdate;
type index_EvalExecutionCreate = EvalExecutionCreate;
type index_EvalExecutionParams = EvalExecutionParams;
type index_EvalExecutionParamsOverride = EvalExecutionParamsOverride;
type index_EvalLlmModelData = EvalLlmModelData;
type index_EvalQuestion = EvalQuestion;
type index_EvalQuestionCreate = EvalQuestionCreate;
type index_EvalQuestionResult = EvalQuestionResult;
type index_ExternallyStoredComponent = ExternallyStoredComponent;
type index_GetPipelineForProjectApiPipelinePipelineIdGetRequest = GetPipelineForProjectApiPipelinePipelineIdGetRequest;
type index_GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest = GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest;
type index_GoogleDocsReader = GoogleDocsReader;
type index_GoogleSheetsReader = GoogleSheetsReader;
type index_HtmlNodeParser = HtmlNodeParser;
type index_HttpValidationError = HttpValidationError;
declare const index_JobNames: typeof JobNames;
type index_JsonNodeParser = JsonNodeParser;
type index_ListProjectsApiProjectGetRequest = ListProjectsApiProjectGetRequest;
declare const index_LlamaParseSupportedFileExtensions: typeof LlamaParseSupportedFileExtensions;
type index_LoadedFile = LoadedFile;
type index_LoadedFilePayload = LoadedFilePayload;
type index_LocalEval = LocalEval;
type index_LocalEvalResults = LocalEvalResults;
type index_LocalEvalSetCreate = LocalEvalSetCreate;
type index_LocalEvalSets = LocalEvalSets;
declare const index_ManagedIngestionStatus: typeof ManagedIngestionStatus;
type index_MarkdownNodeParser = MarkdownNodeParser;
declare const index_MessageRole: typeof MessageRole;
type index_MetricResult = MetricResult;
type index_NotionPageReader = NotionPageReader;
declare const index_ObjectType: typeof ObjectType;
type index_OpenAiEmbedding = OpenAiEmbedding;
declare const index_ParserLanguages: typeof ParserLanguages;
type index_ParsingJob = ParsingJob;
type index_ParsingJobMarkdownResult = ParsingJobMarkdownResult;
type index_ParsingJobTextResult = ParsingJobTextResult;
type index_ParsingUsage = ParsingUsage;
type index_PgVectorStore = PgVectorStore;
type index_PineconeVectorStore = PineconeVectorStore;
type index_Pipeline = Pipeline;
type index_PipelineCreate = PipelineCreate;
type index_PipelineManagedIngestionJobRecord = PipelineManagedIngestionJobRecord;
declare const index_PipelineType: typeof PipelineType;
type index_PipelineUpdate = PipelineUpdate;
type index_PlatformTextNode = PlatformTextNode;
type index_PlatformTextNodeRelationshipsValue = PlatformTextNodeRelationshipsValue;
type index_PlaygroundJobRecord = PlaygroundJobRecord;
type index_PresetRetrievalParams = PresetRetrievalParams;
type index_Project = Project;
type index_ProjectCreate = ProjectCreate;
type index_ProjectUpdate = ProjectUpdate;
type index_PromptMixinPrompts = PromptMixinPrompts;
type index_PromptSpec = PromptSpec;
type index_QdrantVectorStore = QdrantVectorStore;
type index_RawFile = RawFile;
type index_RawFileManagedIngestionJobRecord = RawFileManagedIngestionJobRecord;
type index_ReaderConfig = ReaderConfig;
type index_RelatedNodeInfo = RelatedNodeInfo;
type index_RetrievalParams = RetrievalParams;
type index_RetrieveResults = RetrieveResults;
type index_RssReader = RssReader;
type index_S3Reader = S3Reader;
type index_S3ReaderFileExtractorValue = S3ReaderFileExtractorValue;
type index_SearchPipelinesApiPipelineGetRequest = SearchPipelinesApiPipelineGetRequest;
type index_SentenceSplitter = SentenceSplitter;
type index_SimpleFileNodeParser = SimpleFileNodeParser;
type index_SimpleWebPageReader = SimpleWebPageReader;
type index_SlackReader = SlackReader;
declare const index_StatusEnum: typeof StatusEnum;
type index_StripeWebhookApiBillingWebhookPostRequest = StripeWebhookApiBillingWebhookPostRequest;
type index_SupportedEvalLlmModel = SupportedEvalLlmModel;
declare const index_SupportedEvalLlmModelNames: typeof SupportedEvalLlmModelNames;
type index_TextNode = TextNode;
type index_TextNodeRelationshipsValue = TextNodeRelationshipsValue;
type index_TextNodeWithScore = TextNodeWithScore;
type index_TokenTextSplitter = TokenTextSplitter;
type index_TrafilaturaWebReader = TrafilaturaWebReader;
declare const index_TransformationCategoryNames: typeof TransformationCategoryNames;
type index_UnprocessableEntityError = UnprocessableEntityError;
declare const index_UnprocessableEntityError: typeof UnprocessableEntityError;
type index_ValidationError = ValidationError;
type index_ValidationErrorLocItem = ValidationErrorLocItem;
type index_WeaviateVectorStore = WeaviateVectorStore;
type index_YoutubeTranscriptReader = YoutubeTranscriptReader;
declare namespace index {
  export { type index_ApiKey as ApiKey, type index_ApiKeyCreate as ApiKeyCreate, type index_ApiKeyUpdate as ApiKeyUpdate, type index_AzureOpenAiEmbedding as AzureOpenAiEmbedding, type index_BasePydanticReader as BasePydanticReader, type index_BeautifulSoupWebReader as BeautifulSoupWebReader, type index_BodyUploadFileApiParsingUploadPost as BodyUploadFileApiParsingUploadPost, type index_BodyUpsertRawFileApiPipelinePipelineIdRawFilePut as BodyUpsertRawFileApiPipelinePipelineIdRawFilePut, type index_ChatMessage as ChatMessage, type index_CheckoutSessionCreatePayload as CheckoutSessionCreatePayload, type index_ChromaVectorStore as ChromaVectorStore, type index_CodeSplitter as CodeSplitter, index_ConfigurableDataSinkNames as ConfigurableDataSinkNames, index_ConfigurableDataSourceNames as ConfigurableDataSourceNames, type index_ConfigurableTransformationDefinition as ConfigurableTransformationDefinition, index_ConfigurableTransformationNames as ConfigurableTransformationNames, type index_ConfiguredTransformationItem as ConfiguredTransformationItem, type index_ConfiguredTransformationItemComponent as ConfiguredTransformationItemComponent, type index_ConfiguredTransformationItemComponentOne as ConfiguredTransformationItemComponentOne, type index_CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest as CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest, type index_CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest as CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest, type index_CustomerPortalSessionCreatePayload as CustomerPortalSessionCreatePayload, type index_DataSink as DataSink, type index_DataSinkComponent as DataSinkComponent, type index_DataSinkComponentOne as DataSinkComponentOne, type index_DataSinkCreate as DataSinkCreate, type index_DataSinkCreateComponent as DataSinkCreateComponent, type index_DataSinkCreateComponentOne as DataSinkCreateComponentOne, type index_DataSinkDefinition as DataSinkDefinition, type index_DataSinkUpdate as DataSinkUpdate, type index_DataSinkUpdateComponent as DataSinkUpdateComponent, type index_DataSinkUpdateComponentOne as DataSinkUpdateComponentOne, type index_DataSource as DataSource, type index_DataSourceComponent as DataSourceComponent, type index_DataSourceComponentOne as DataSourceComponentOne, type index_DataSourceCreate as DataSourceCreate, type index_DataSourceCreateComponent as DataSourceCreateComponent, type index_DataSourceCreateComponentOne as DataSourceCreateComponentOne, type index_DataSourceDefinition as DataSourceDefinition, type index_DataSourceLoadJobRecord as DataSourceLoadJobRecord, type index_DataSourceManagedIngestionJobRecord as DataSourceManagedIngestionJobRecord, type index_DataSourceUpdate as DataSourceUpdate, type index_DataSourceUpdateComponent as DataSourceUpdateComponent, type index_DataSourceUpdateComponentOne as DataSourceUpdateComponentOne, type index_DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest as DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest, type index_DiscordReader as DiscordReader, type index_Document as Document, type index_DocumentGroup as DocumentGroup, type index_DocumentRelationshipsValue as DocumentRelationshipsValue, type index_EvalDataset as EvalDataset, type index_EvalDatasetCreate as EvalDatasetCreate, type index_EvalDatasetJobRecord as EvalDatasetJobRecord, type index_EvalDatasetUpdate as EvalDatasetUpdate, type index_EvalExecutionCreate as EvalExecutionCreate, type index_EvalExecutionParams as EvalExecutionParams, type index_EvalExecutionParamsOverride as EvalExecutionParamsOverride, type index_EvalLlmModelData as EvalLlmModelData, type index_EvalQuestion as EvalQuestion, type index_EvalQuestionCreate as EvalQuestionCreate, type index_EvalQuestionResult as EvalQuestionResult, type index_ExternallyStoredComponent as ExternallyStoredComponent, type index_GetPipelineForProjectApiPipelinePipelineIdGetRequest as GetPipelineForProjectApiPipelinePipelineIdGetRequest, type index_GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest as GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest, type index_GoogleDocsReader as GoogleDocsReader, type index_GoogleSheetsReader as GoogleSheetsReader, type index_HtmlNodeParser as HtmlNodeParser, type index_HttpValidationError as HttpValidationError, index_JobNames as JobNames, type index_JsonNodeParser as JsonNodeParser, type index_ListProjectsApiProjectGetRequest as ListProjectsApiProjectGetRequest, index_LlamaParseSupportedFileExtensions as LlamaParseSupportedFileExtensions, type index_LoadedFile as LoadedFile, type index_LoadedFilePayload as LoadedFilePayload, type index_LocalEval as LocalEval, type index_LocalEvalResults as LocalEvalResults, type index_LocalEvalSetCreate as LocalEvalSetCreate, type index_LocalEvalSets as LocalEvalSets, index_ManagedIngestionStatus as ManagedIngestionStatus, type index_MarkdownNodeParser as MarkdownNodeParser, index_MessageRole as MessageRole, type index_MetricResult as MetricResult, type index_NotionPageReader as NotionPageReader, index_ObjectType as ObjectType, type index_OpenAiEmbedding as OpenAiEmbedding, index_ParserLanguages as ParserLanguages, type index_ParsingJob as ParsingJob, type index_ParsingJobMarkdownResult as ParsingJobMarkdownResult, type index_ParsingJobTextResult as ParsingJobTextResult, type index_ParsingUsage as ParsingUsage, type index_PgVectorStore as PgVectorStore, type index_PineconeVectorStore as PineconeVectorStore, type index_Pipeline as Pipeline, type index_PipelineCreate as PipelineCreate, type index_PipelineManagedIngestionJobRecord as PipelineManagedIngestionJobRecord, index_PipelineType as PipelineType, type index_PipelineUpdate as PipelineUpdate, type index_PlatformTextNode as PlatformTextNode, type index_PlatformTextNodeRelationshipsValue as PlatformTextNodeRelationshipsValue, type index_PlaygroundJobRecord as PlaygroundJobRecord, type index_PresetRetrievalParams as PresetRetrievalParams, type index_Project as Project, type index_ProjectCreate as ProjectCreate, type index_ProjectUpdate as ProjectUpdate, type index_PromptMixinPrompts as PromptMixinPrompts, type index_PromptSpec as PromptSpec, type index_QdrantVectorStore as QdrantVectorStore, type index_RawFile as RawFile, type index_RawFileManagedIngestionJobRecord as RawFileManagedIngestionJobRecord, type index_ReaderConfig as ReaderConfig, type index_RelatedNodeInfo as RelatedNodeInfo, type index_RetrievalParams as RetrievalParams, type index_RetrieveResults as RetrieveResults, type index_RssReader as RssReader, type index_S3Reader as S3Reader, type index_S3ReaderFileExtractorValue as S3ReaderFileExtractorValue, type index_SearchPipelinesApiPipelineGetRequest as SearchPipelinesApiPipelineGetRequest, type index_SentenceSplitter as SentenceSplitter, type index_SimpleFileNodeParser as SimpleFileNodeParser, type index_SimpleWebPageReader as SimpleWebPageReader, type index_SlackReader as SlackReader, index_StatusEnum as StatusEnum, type index_StripeWebhookApiBillingWebhookPostRequest as StripeWebhookApiBillingWebhookPostRequest, type index_SupportedEvalLlmModel as SupportedEvalLlmModel, index_SupportedEvalLlmModelNames as SupportedEvalLlmModelNames, type index_TextNode as TextNode, type index_TextNodeRelationshipsValue as TextNodeRelationshipsValue, type index_TextNodeWithScore as TextNodeWithScore, type index_TokenTextSplitter as TokenTextSplitter, type index_TrafilaturaWebReader as TrafilaturaWebReader, index_TransformationCategoryNames as TransformationCategoryNames, index_UnprocessableEntityError as UnprocessableEntityError, type index_ValidationError as ValidationError, type index_ValidationErrorLocItem as ValidationErrorLocItem, type index_WeaviateVectorStore as WeaviateVectorStore, type index_YoutubeTranscriptReader as YoutubeTranscriptReader, index$7 as apiKey, index$1 as billing, index$2 as componentDefinition, index$a as dataSink, index$9 as dataSource, index$4 as eval, index$8 as health, index$3 as parsing, index$5 as pipeline, index$6 as project };
}

export { type DataSourceDefinition as $, type ApiKey as A, type EvalExecutionCreate as B, type CreatePlaygroundPipelineApiPipelinePipelineIdPlaygroundPostRequest as C, type DataSinkCreate as D, type EvalDataset as E, type EvalQuestionResult as F, type GetPipelineForProjectApiPipelinePipelineIdGetRequest as G, type DataSourceManagedIngestionJobRecord as H, type PipelineManagedIngestionJobRecord as I, type RetrievalParams as J, type RetrieveResults as K, type ListProjectsApiProjectGetRequest as L, type RawFile as M, type EvalDatasetUpdate as N, type EvalQuestion as O, type Project as P, type EvalQuestionCreate as Q, type RawFileManagedIngestionJobRecord as R, type SearchPipelinesApiPipelineGetRequest as S, type SupportedEvalLlmModel as T, LlamaParseSupportedFileExtensions as U, type BodyUploadFileApiParsingUploadPost as V, type ParsingJob as W, type ParsingUsage as X, type ParsingJobTextResult as Y, type ParsingJobMarkdownResult as Z, type ConfigurableTransformationDefinition as _, type ApiKeyCreate as a, type PgVectorStore as a$, type DataSinkDefinition as a0, type CheckoutSessionCreatePayload as a1, type CustomerPortalSessionCreatePayload as a2, type StripeWebhookApiBillingWebhookPostRequest as a3, index as a4, PlatformApiError as a5, index$a as a6, index$9 as a7, index$8 as a8, index$7 as a9, type DataSinkCreateComponent as aA, type DataSourceComponentOne as aB, type DataSourceComponent as aC, type DataSourceCreateComponentOne as aD, type DataSourceCreateComponent as aE, type DiscordReader as aF, type DocumentRelationshipsValue as aG, type Document as aH, type DocumentGroup as aI, type EvalExecutionParams as aJ, type EvalExecutionParamsOverride as aK, type EvalLlmModelData as aL, type ExternallyStoredComponent as aM, type GoogleDocsReader as aN, type GoogleSheetsReader as aO, type HtmlNodeParser as aP, type HttpValidationError as aQ, type JsonNodeParser as aR, JobNames as aS, type LocalEval as aT, ManagedIngestionStatus as aU, type MarkdownNodeParser as aV, MessageRole as aW, type MetricResult as aX, type NotionPageReader as aY, ObjectType as aZ, type OpenAiEmbedding as a_, index$6 as aa, index$5 as ab, index$4 as ac, index$3 as ad, index$2 as ae, index$1 as af, type DataSinkUpdateComponentOne as ag, type DataSinkUpdateComponent as ah, type DataSourceUpdateComponentOne as ai, type DataSourceUpdateComponent as aj, type BodyUpsertRawFileApiPipelinePipelineIdRawFilePut as ak, type AzureOpenAiEmbedding as al, type BasePydanticReader as am, type BeautifulSoupWebReader as an, type ChatMessage as ao, type ChromaVectorStore as ap, type CodeSplitter as aq, ConfigurableDataSinkNames as ar, ConfigurableDataSourceNames as as, ConfigurableTransformationNames as at, type ConfiguredTransformationItemComponentOne as au, type ConfiguredTransformationItemComponent as av, type ConfiguredTransformationItem as aw, type DataSinkComponentOne as ax, type DataSinkComponent as ay, type DataSinkCreateComponentOne as az, type ApiKeyUpdate as b, ParserLanguages as b0, type PineconeVectorStore as b1, PipelineType as b2, type PlatformTextNodeRelationshipsValue as b3, type PresetRetrievalParams as b4, type PromptSpec as b5, type QdrantVectorStore as b6, type ReaderConfig as b7, type RelatedNodeInfo as b8, type RssReader as b9, type S3ReaderFileExtractorValue as ba, type S3Reader as bb, type SentenceSplitter as bc, type SimpleFileNodeParser as bd, type SimpleWebPageReader as be, type SlackReader as bf, StatusEnum as bg, SupportedEvalLlmModelNames as bh, type TextNodeRelationshipsValue as bi, type TextNode as bj, type TextNodeWithScore as bk, type TokenTextSplitter as bl, type TrafilaturaWebReader as bm, TransformationCategoryNames as bn, type ValidationErrorLocItem as bo, type ValidationError as bp, type WeaviateVectorStore as bq, type YoutubeTranscriptReader as br, UnprocessableEntityError as bs, type DataSink as c, type DataSinkUpdate as d, type DataSourceCreate as e, type DataSource as f, type DataSourceUpdate as g, type DataSourceLoadJobRecord as h, type ProjectCreate as i, type ProjectUpdate as j, type PipelineCreate as k, type Pipeline as l, type EvalDatasetCreate as m, type LocalEvalSetCreate as n, type LocalEvalResults as o, type LocalEvalSets as p, type PromptMixinPrompts as q, type PipelineUpdate as r, type LoadedFile as s, type LoadedFilePayload as t, type DeployPlaygroundPipelineApiPipelinePipelineIdDeployPostRequest as u, type PlaygroundJobRecord as v, type CreatePlaygroundJobApiPipelinePipelineIdPlaygroundJobPostRequest as w, type GetPlaygroundJobResultApiPipelinePipelineIdPlaygroundJobResultGetRequest as x, type PlatformTextNode as y, type EvalDatasetJobRecord as z };
